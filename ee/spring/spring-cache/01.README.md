## 使用场景: `缓存只保证最终一致性`

1. 即时性, 数据一致性要求不高: _物流信息_, _商品分类_
2. 访问量大, 但是更新频率不高的数据`[读多写少]`: _商品信息_
3. 遇到实时性一致性要求超高的, 应该直接查询数据库

## 缓存分类

1. 本地缓存

   - 比如存在 Map 中, 只适用于单体应用
   - 分布式下存在数据不一致问题

2. 分布式缓存

### 本地缓存锁

![avatar](/static/image/db/redis-standalone.png)

1. 确保以下 3 个动作是原子的, 二期是在锁住的方法内
   - query from redis cache
   - query database
   - put result to redis cache.-

### 分布式缓存锁

![avatar](/static/image/db/redis-distribute.png)

1. core: 在指定地方占一个位置, 占上位置就可以执行逻辑, 否则等待
2. 加锁[原子操作]: `SET K V NX EX`
3. 释放锁[原子操作]: `if redis.call('get',KEYS[1]) == ARGV[1] then return redis.call('del',KEYS[1]) else return 0 end` 返回值时 Long
4. 只能删除自己的锁, 业务逻辑时间长导致的 redis key 过期而引起的删除别人持有的锁: 可以适当的设置 redis key 过期时间长一点
5. processor

   ![avatar](/static/image/db/redis-distribute-v1.png)
   ![avatar](/static/image/db/redis-distribute-v2.png)
   ![avatar](/static/image/db/redis-distribute-v3.png)
   ![avatar](/static/image/db/redis-distribute-v4.png)

### 分布式缓存框架 Redisson

1. Redisson 是一个在 Redis 的基础上实现的 Java 驻内存数据网格[In-Memory Data Grid]
2. 它不仅提供了一系列的分布式的 Java 常用对象, 还提供了许多分布式服务
   - BitSet
   - Set
   - Multimap
   - SortedSet
   - Map
   - List
   - Queue
   - BlockingQueue
   - Deque
   - BlockingDeque
   - Semaphore
   - Lock
   - AtomicLong
   - CountDownLatch
   - Publish / Subscribe
   - Bloom filter
   - Remote service
   - Spring cache
   - Executor service
   - Live Object service
   - Scheduler service
3. Redisson 提供了使用 Redis 的最简单和最便捷的方法
4. Redisson 的宗旨是促进使用者对 Redis 的关注分离[Separation of Concern], 从而让使用者能够将精力更集中地放在处理业务逻辑上
5. Redisson 实现类 JUC 的锁

#### lock

1. redisson 获取锁时时阻塞等待, 而不是自旋
   - lock.lock(long leaseTime, TimeUnit unit): 不会自动续期, 过期之后就会被删除
2. redisson 获取分布式锁, 名字相同即为一把锁
3. redisson 不会出现死锁: 锁的过期时间默认为 30s
4. redisson 锁的自动续期: 看门狗
   - lock(-1, TimeUnit unit): 获取锁, 设置默认的时间哪位 30s, 并在占锁成功之后设置一个定时器对锁进行续期[30s/3=10s]
5. 最佳实战:
   - `lock(20, TimeUtil.SECONDS): 省下了续期操作, 手动解锁`

#### tryLock

1. 最大等待时间, 否则就放弃
   - boolean b = lock.tryLock(100, 10, TimeUnit.SECONDS): `最大等待100s, 最长持有时间 10s就释放锁`

#### FairLock: 公平锁

1. 有顺序的获取锁

   - RLock lock = redisson.getFairLock("anyLock")

#### 读写所: `经常读, 很少写`

1. 允许多个读锁和一个写锁处于加锁状态

   ```java
   RReadWriteLock rwlock = redisson.getReadWriteLock("rwlock");
   rwlock.readLock().lock();
   rwlock.writeLock().lock();
   ```

2. 读数据使用读锁, 写数据使用写锁

3. 读写锁也会自动续期, 但是可以读到最新的数据[修改期间会阻塞所有操作]
   - 写 + 读: 等待写锁释放
   - 写 + 写: 阻塞写
   - 读 + 写: 等待写锁释放
   - 读 + 读: 无锁, 只会记录所有的读锁, 都能加锁成功

#### 闭锁

1. demo: 等 5 个班都锁门, 才可以锁学校大门
2. getCountDownLatch Key 过期时间 -1
3. latch count 为 0, 最小值为 0
4. getCountDownLatch 会阻塞, 直到 count=0 时才执行

#### 信号量: 限流

1. demo: 停车场, 限流
2. park 过期时间为 -1
3. 当没有车位时, 停车动作会被阻塞, 直到有车离开[空出车位]
4. 车离开没有限制
5. 限流:

   - 信号量为 1000, 当线程来时取一个, 执行结束则返还, 1000 个信号量被取完则线程需要等待
   - code

     ```java
     boolean acquire = park.tryAcquire();
     if(acquire) {
        // buz logic
     } else {
        return "当前流量过大";
     }
     ```

## 缓存数据的一致性问题

1. 双写模式: `第二个线程写缓存先执行了导致`的不一致

   ![avatar](/static/image/db/redis-data-consistency.png)

2. 失效模式:

   - `第三个执行查询写入 redis 快于第二个, 会导致多查数据库`
   - `1 删除结束, 3 进行数据库查询, 2 更新了数据库并删除了缓存, 3 开始写如redis, 此时的数据就不会死最新的[2操作后的数据时最新的]`

   ![avatar](/static/image/db/redis-data-consistency-v2.png)

   - solution: 可以使用读写锁, 2 在写时, 3 读会被阻塞

3. 解决方案

   - 如果时用户维度的数据(订单数据, 用户数据), 并发几率小, `缓存+过期时间+失效模式` 就可以了
   - 如果时菜单, 商品介绍等数据[可以长时间数据不一致]: `canal-binlog`
   - 要求一致性高的可以考虑加锁: `读写锁`
   - 遇到实时性一致性要求超高的, 应该直接查询数据库

## 布隆过滤器: 基于概率的数据结构

1. 当布隆过滤器说某个值存在时,这个值可能不存在; 当它说不存在时, 那就肯定不存在
2. 特点

   - 一个非常大的二进制位数组(数组中只存在 0 和 1）
   - 拥有若干个哈希函数(Hash Function）
   - 在空间效率和查询效率都非常高
   - 布隆过滤器不会提供删除方法, 在代码维护上比较困难。

3. 要提高布隆过滤器的准确率, 就要说到影响它的三个重要因素:
   - 哈希函数的好坏
   - 存储空间大小
   - 哈希函数个数

## Spring Cache

1. 相关概念

   - CacheManager
   - Cache
   - store: K-V

2. Spring-Cache 的不足之处

   - 读模式
     1. 缓存穿透: 查询一个 null 数据。解决方案: 缓存空数据, 可通过 spring.cache.redis.cache-null-values=true
     2. 缓存击穿: 大量并发进来同时查询一个正好过期的数据。解决方案: 加锁 ? 默认是无加锁的;
     3. 使用 sync = true 来解决击穿问题
     4. 缓存雪崩: 大量的 key 同时过期。解决: 加随机时间。加上过期时间
   - 写模式: 缓存与数据库一致
     1. 读写加锁
     2. 引入 Canal,感知到 MySQL 的更新去更新 Redis
     3. 读多写多, 直接去数据库查询就行
   - 总结:

     1. 常规数据(读多写少, 即时性, 一致性要求不高的数据, 完全可以使用 Spring-Cache):
     2. 写模式(只要缓存的数据有过期时间就足够了)
     3. 特殊数据: 特殊设计

## 常见的问题

1. 缓存穿透: 查询一个不存在的 Key, 缓存不能命中, 数据库也没有记录[无法缓存], 导致每次都要查数据库, `缓存失效`
   - solution: 缓存 Null 且加`短暂的过期时间`
   - 做严格的限制, 金狼过滤无效的请求
   - 布隆过滤器: 将所有的 key 都维护在其中, 请求之前先判断一下
2. 缓存击穿: 某个 Key 是高频热点, 但是在大并发下下缓存过期了

   - solution: 加锁, 大并发下只让一个线程去查数据库, 其他人等待, 查到以后释放锁; 其他线程获取到锁, 先查询缓存
   - 数据预热

3. 缓存雪崩: 大量的 Key 的同时失效, 请求压力都转到了数据库, redis 挂了

   - solution: 在原有的失效时间上添加一个随机数以降低重复概率
   - redis 高可用
   - 限流降级: 缓存失效后, 通过加锁或者队列来控制读数据库写缓存的线程数量, 对某个 key 只允许一个线程查询数据和写缓存, 其他线程等待。

4. 可以考虑在业务层面上解决 12306

## issue list

1. OutOfDirectMemoryError:

   - root cause: lettuce's bug, 内存没有得到及时的释放, netty 如果不设置堆外内存则会使用 `-Xmx100m`

   ```java
   // io.netty.util.internal.PlatformDependent
   logger.debug("-Dio.netty.maxDirectMemory: {} bytes", maxDirectMemory);
   DIRECT_MEMORY_LIMIT = maxDirectMemory >= 1 ? maxDirectMemory : MAX_DIRECT_MEMORY;

   private static void incrementMemoryCounter(int capacity) {
       if (DIRECT_MEMORY_COUNTER != null) {
           long newUsedMemory = DIRECT_MEMORY_COUNTER.addAndGet(capacity);
           if (newUsedMemory > DIRECT_MEMORY_LIMIT) {
               DIRECT_MEMORY_COUNTER.addAndGet(-capacity);
               throw new OutOfDirectMemoryError("failed to allocate " + capacity
                       + " byte(s) of direct memory (used: " + (newUsedMemory - capacity)
                       + ", max: " + DIRECT_MEMORY_LIMIT + ')');
           }
       }
   }
   ```

   - solution:
     1. `-Dio.netty.maxDirectMemory`: 长时间运行还是有问题的, 本质还是内存没有得到及时的释放
     2. 使用 jedis,
     3. 升级 lettuce: `5.2.2.RELEASE`
