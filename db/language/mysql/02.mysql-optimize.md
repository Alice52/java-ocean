<!--
 该笔记形成于阅读 `MySQL 实战 45 讲`
-->

## MySQL 45 Docs

### 1. MySQL Server 结构

1. Connectors: _用户的权限在次就确定了, 所以修改后需要重新连接才有效_
2. Service: SQL Interface, 连接器, 查询缓存, 分析器, 优化器, 执行器
3. Engines: `插件式`
4. Store

### 2. SQL 查询流程

1. Client
2. Connectors

   - `show processlist` 可以看到所有的连接信息, 默认 `wait_timeout` 8 小时后断开
   - 但是全部使用长连接后, 有些时候 MySQL 占用内存涨得特别快: 是因为 `MySQL 在执行过程中临时使用的内存是管理在连接对象里面的`, 这些资源会在连接断开的时候才释放
   - solution: 执行占用内存的大查询后断开连接或者执行 `mysql_reset_connection` 来重新初始化连接资源[这个过程不需要重连和重新做权限验证]

3. ~~Cache~~

   - 缓存失效时针对表的, 不建议使用
   - sql_no_cache/sql_cache

4. Parser: 让 MySQL 知道做什么

   - `select SQL_CACHE * from T where ID=10;`
   - 词法分析: MySQL 从你输入的 "select" 这个关键字识别出来, 这是一个查询语, 它也要把字符串 T 识别成表名 T , 把字符串 ID 识别成 列 ID
   - 语法分析: 是否满足 MySQL 语法

5. Optimizer: MySQL 决定怎么做

   - 优化器是在表里面有多个索引的时候, 决定使用哪个索引;
   - 或者在一个语句有多表关联的时候, 决定各个表的连接顺序
   - ...

6. Executor: 做事情

   - 判断有无表操作的权限
   - 打开表执行器会根据表的引擎定义, 去使用这个引擎提供的接口获取数据
   - 比如 `4` 的 SQL 且 ID 字段没有索引, 执行器的执行流程
     1. 调用 InnoDB 引擎接口取这个表的第一行, 判断 ID 值是不是 10, 如果不是则跳过, 如果是则将这行存在结果集中;
     2. 调用引擎接口取下一行 , 重复相同的判断逻辑, 直到取到这个表的最后一行
     3. 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端

   ![avatar](/static/image/db/mysql-sql-execute-flow.png)

### 3. SQL 更新流程

1. Client
2. Connectors
3. ~~Cache~~: 使该表的缓存全部失效
4. Parser: 分析器会通过词法和语法解析知道这是一条更新语句
5. Optimizer: 优化器决定要使用 ID 这个索引
6. Executor: 执行器负责具体执行, 找到这一行, 然后更新
7. Engines:

   - 执行器先`找引擎取` ID=2 这一行: ID 是主键, 引擎直接用树搜索找到这一行, 如果 ID=2 这一行所在的数据页本来就在内存中, 就直接`返回给执行器`; 否则, 需要先从磁盘读入内存, 然后再返回
   - `执行器`拿到引擎给的行数据, 把这个值`加上 1`, 比如原来是 N , 现在就是 N+1 , 得到新的一行数据, 再`调用引擎接口写入这行新数据`
   - `引擎`将这行新数据`更新到内存`中, 同时将这个更新操作`记录到 redo log 里面`, 此时 **redo log 处于 prepare 状态**, 然后`告知执行器执行完成了`, 随时可以提交事务
   - **执行器生成这个操作的 binlog , 并把 binlog 写入磁盘**
   - **执行器调用引擎的提交事务接口, 引擎把刚刚写入的 redo log 改成提交 commit 状态, 更新完成**

8. 与查询流程不一样的是, 更新流程还涉及两个重要的日志模块: `redo log 和 binlog`

   - redo log: `innodb` 中是一快可`重复利用`的`顺序读写`的`固定大小`的`磁盘空间`, 是一种 `Write-Ahead Logging[先写日志]`, `物理日志`[数据页上的修改]

     ![avatar](/static/image/db/mysql-wal.png)

     - write pos 是当前记录的位置, 一边写一边后移, 写到第 3 号文件末尾后就回到 0 号文件开头
     - checkpoint 是当前要擦除的位置, 擦除记录前要把记录更新到数据文件
     - write pos 和 checkpoint 之间的是空着的部分, 可以用来记录新的操作.
     - `如果 write pos 追上 checkpoint, 这时候不能再执行新的更新`, 得停下来等待 checkpoint 推进

   - binlog: service 层的归档日志, 可选择性打开的逻辑日志[记录语句逻辑], 多用于 SM, 无限空间, 追加记录
     - sync_binlog

   ![avatar](/static/image/db/mysql-update-flow.png)

9. crash-safe: 有了 redo log , InnoDB 就可以保证即使数据库发生异常重启, 之前提交的记录都不会丢失
10. `简单说, redo log 和 binlog 都可以用于表示事务的提交状态, 而两阶段提交就是让这两个状态保持逻辑上的一致`

### 4. 恢复到指定的时刻的数据

1. 找到最近的一次全量备份, 从这个备份恢复到临时库
2. 从备份的时间点开始, 将备份的 binlog 依次取出来, 重放到指定时刻

### 5. redo-log 日志需要两阶段提交

1. 存储引擎更新数据时会更新内存数据且写 redo log, 并设置为 prepare 阶段
2. 之后会告诉执行器, 执行完成, 执行器会写 bin-log
3. 之后执行器会调用存储引擎提交事务接口, 引擎把刚刚写入的 redo log 改成提交 commit 状态
4. 此时更新操作就算结束, 后续会将 redo log 的内容持久化的磁盘

5. why？

   - 由于 redo log 和 binlog 是两个独立的逻辑
   - `先写 redo log 后写 binlog`: 假设在 redo log 写完, binlog 还没有写完的时候, MySQL 进程异常重启. 由于我们前面说过的, redo log 写完之后, 系统即使崩溃, 仍然能够把数据恢复回来, 所以恢复后这一行的值是 1 . 但是由于 binlog 没写完就 crash 了, 这时候 binlog 里面就没有记录这个语句. 因此, 之后备日志的时候, 存起来的 binlog 里面就没有这条语句. 然后你会发现, 如果需要用这个 binlog 来恢复临时库的话, 由于这个语句的 binlog 丢失, 这个临时库就会少了这一次更新, 恢复出来的这一行 c 的值就是 0 , 与原库的值不同.
   - `先写 binlog 后写 redo log`: 如果在 binlog 写完之后 crash, 由于 redo log 还没写, 崩溃恢复以后这个事务无效, 所以这一行 c 的值是 0. 但是 binlog 里面已经记录了 _把 c 从 0 改成 1_ 这个日志. 所以, 在之后用 binlog 来恢复的时候就多了一个事务出来, 恢复出来的这一行 c 的值就是 1 , 与原库的值不同.

### 6. 强数据一致性相关参数设置

1. `bin-log: sync_binlog`

   - 1[默认值]每次事务提交都调用 fysnc
   - 0 则每次事务提交只是 write 到 page cache, 没有立即调用 fsync
   - N 表示第 N 个事务时才调用 fsync

2. `redo-log: innodb_flush_log_at_trx_commit`

   - 1: 每次事务的 redo log 都直接持久化到磁盘
   - 0: 表示写入 innodb 的 logbuffer, 每秒调用一次 fsync 刷入磁盘
   - 3: 表示直接写入 OS buffer, 每秒调用 fsync 刷入磁盘

### 7. 可重复读的场景

1. 假设你在管理一个个人银行账户表, 一个表存了每个月月底的余额, 一个表存了账单明细
2. 这时候你要做数据校对, 也就是判断上个月的余额和当前余额的差额, 是否与本月的账单明细一致
3. 你一定希望在校对过程中, 即使有用户发生了一笔新的交易, 也不影响你的校对结果
4. 这时候使用 可重复读 隔离级别就很方便

### 8. 可重复读的实现: `mvcc+undo-log`

1. 在 MySQL 中, 实际上每条记录在更新的时候都会同时记录一条回滚操作
2. 记录上的最新值, 通过回滚操作, 都可以得到前一个状态的值
3. 假设一个值从 1 被按顺序改成了 2、3、4, 在回滚

   ![avatar](/static/image/db/mysql-isolation-rr.png)

4. 当前值是 4, 但是在查询这条记录的时候, 不同时刻启动的事务会有不同的 read-view

   - 如图中看到的, 在视图 A, B, C 里面, 这一个记录的值分别是 1, 2, 4,
   - 同一条记录在系统中可以存在多个版本, 就是数据库的多版本并发控制(MVCC)
   - 对于 read-view A , 要得到 1, 就必须将当前值依次执行图中所有的回滚操作得到

5. 即使现在有另外一个事务正在将 4 改成 5, 这个事务跟 read-view A, B, C 对应的事务是不会冲突的

6. 当没有事务再需要用到这些回滚日志时, 回滚日志会被删除
   - `当系统里没有比这个回滚日志更早的 read-view 的时候`

### 9. 长事务问题

1. 长事务意味着系统里面会存在很老的事务视图

   - 由于这些事务随时可能访问数据库里面的任何数据, 所以这个事务提交之前
   - 数据库里面它可能用到回滚记录都必须保留, 这就会导致大量占用存储空间

2. MySQL 5.5 之前回滚日志是跟数据字典一起放在 ibdata 文件里的, 会导致其很大很大
3. 长事务还占用锁资源

### 10. 页分裂&合并

1. 页分裂: 如果所在的数据页已经满了, 根据 B+ 树的算法, 这时候需要申请一个新的数据页, ~~然后

   - 性能自然会受影响
   - 页分裂操作还影响数据页的利用率: 原本放在一个页的数据, 现在分到两个页中

2. 当相邻两个页由于删除了数据, 利用率很低之后, 会将数据页做合并

### 11. 用业务字段直接做主键

1. 只有一个索引: 由于没有其他索引, 所以也就不用考虑其他索引的叶子节点大小的问题
2. 该索引必须是唯一索引: 典型的 KV 场景

### 12. 非聚簇索引的查询过程

```sql
create table T (
   ID int primary key,
   k int NOT NULL DEFAULT 0,
   s varchar(16) NOT NULL DEFAULT '',
   index k(k)
) engine=InnoDB;

insert into T values(100, 1,  'aa'), (200, 2, 'bb'), (300, 3, 'cc'), (500, 5, 'ee'), (600, 6, 'ff'), (700, 7, 'gg');

-- execute flow
select * from T where k between 3 and 5
```

1. 在 k 索引树上找到 k=3 的记录, 取得 ID = 300
2. 再到 ID 索引树查到 ID=300 对应的 R3
3. 在 k 索引树取下一个值 k=5 ，取得 ID=500
4. 再回到 ID 索引树查到 ID=500 对应的 R4
5. 在 k 索引树取下一个值 k=6 ，不满足条件，循环结束。

   ![avatar](/static/image/db/mysql-index-query.png)

6. `select ID from T where k between 3 and 5`:

   - 在引擎内部使用覆盖索引在索引 k 上其实读了三个记录
   - R3~R5[对应的索引 k 上的记录项], 但是对于 MySQL 的 Server 层来说, 它就是找引擎拿到了两条记录，因此 MySQL 认为扫描行数是 2

### 13. 索引

```sql
CREATE TABLE `tuser` (
   `id` int(11) NOT NULL,
   `id_card` varchar(32) DEFAULT NULL,
   `name` varchar(32) DEFAULT NULL,
   `age` int(11) DEFAULT NULL,
   `ismale` tinyint(1) DEFAULT NULL,
   PRIMARY KEY (`id`),
   KEY `id_card` (`id_card`),
   KEY `name_age` (`name`,`age`)
) ENGINE=InnoDB
```

![avatar](/static/image/db/mysql-index-practice.png)

1. 覆盖索引
2. 联合索引

   - 第一原则是, 如果通过调整顺序, 可以少维护一个索引, 那么这个顺序往往就是需要优先考虑采用的
   - 考虑的原则就是空间

3. 回表
4. 最左前缀原则
5. 索引下推

   ```sql
   -- 以市民表的联合索引[name, age]
   -- 检索出表中名字第一个字是张, 而且年龄是10岁的所有男孩
   -- name 会用到索引
   select * from tuser where name like '张%' and age=10 and ismale=1;
   ```

   - MySQL 5.6 之前, 只能从 ID3 开始一个个回表: `到主键索引上找出数据行, 再对比字段值`
   - MySQL 5.6 引入的`索引下推优化[index condition pushdown], 可以在索引遍历过程中, 对索引中包含的字段先做判断, 直接过滤掉不满足条件的记录, 减少回表次数`

   ![avatar](/static/image/db/mysql-index-practice-down.png)
   ![avatar](/static/image/db/mysql-index-practice-down-v5.5.png)

### 14. 索引分析案例

```sql
CREATE TABLE `geek` (
   `a` int(11) NOT NULL,
   `b` int(11) NOT NULL,
   `c` int(11) NOT NULL,
   `d` int(11) NOT NULL,
   PRIMARY KEY (`a`,`b`),
   KEY `c` (`c`),
   KEY `ca` (`c`,`a`),
   KEY `cb` (`c`,`b`)
) ENGINE=InnoDB;
```

1. 由于历史原因, 这个表需要 a, b 做联合主键, 所以 `index(a, b)`
2. ~~所以 `index(c, a)` for `select * from geek where c=N order by a limit 1;`?~~： this is no use.
3. 所以 `index(c, b)` for `select * from geek where c=N order by b limit 1;`?
4. 分析索引

   - `index(a, b)`:

     |  a  |  b  |  c  |  d  |
     | :-: | :-: | :-: | :-: |
     |  1  |  2  |  3  |  d  |
     |  1  |  3  |  2  |  d  |
     |  1  |  4  |  3  |  d  |
     |  2  |  1  |  3  |  d  |
     |  2  |  2  |  2  |  d  |
     |  2  |  3  |  4  |  d  |

   - `index(c)`: `same as index(c, a)`

     |  c  |  a  |  b  |
     | :-: | :-: | :-: |
     |  2  |  1  |  3  |
     |  2  |  2  |  2  |
     |  3  |  1  |  2  |
     |  3  |  1  |  4  |
     |  3  |  2  |  1  |
     |  4  |  2  |  3  |

   - ~~`index(c, a)`~~: `same as index(c)`

     |  c  |  a  |  b  |
     | :-: | :-: | :-: |
     |  2  |  1  |  3  |
     |  2  |  2  |  2  |
     |  3  |  1  |  2  |
     |  3  |  1  |  4  |
     |  3  |  2  |  1  |
     |  4  |  2  |  3  |

   - `index(c, b)`

     |  c  |  a  |  b  |
     | :-: | :-: | :-: |
     |  2  |  2  |  2  |
     |  2  |  3  |  1  |
     |  3  |  1  |  2  |
     |  3  |  2  |  1  |
     |  3  |  4  |  1  |
     |  4  |  3  |  2  |

### 15. 重建索引

1. 索引可能因为删除, 或者页分裂等原因，导致数据页有空洞
2. 重建索引的过程会创建一个新的索引, 把数据按顺序插入, 这样页面的利用率最高, 也就是索引更紧凑、更省空间

### 16. 全局锁

1. 全局读锁~~`Flush tables with read lock (FTWRL)`~~: 典型使用场景做全库逻辑备份`mysqldump`
2. **优先使用:** `mysqldump single-transaction` 方法只适用于所有的表使用事务引擎的库

   - 导数据之前就会启动一个事务, 来确保拿到一致性视图
   - 而由于 MVCC 的支持, 这个过程中数据是可以正常更新的
   - `如果有的表使用了不支持事务的引擎，那么备份就只能通过 FTWRL 方法`
   - FTWRL 命令之后由于客户端发生异常断开, 那么 MySQL 会自动释放这个全局锁, 整个库回到可以正常更新的状态

3. 表加字段操作, 都是会被锁住的: 全局锁

### 17. 表级别的锁: `lock tables … read/write unlock tables`

1. 分类

   - 表锁
   - 元数据锁

2. 元数据锁

   - 在 MySQL 5.5 版本中引入了 MDL, 当对一个表做增删改查操作的时候, 加 MDL 读锁
   - 要对表做结构变更操作的时候, 加 MDL 写锁

3. MDL 锁导致一个小表加个字段, 导致整个库挂了

   ![avatar](/static/image/db/mysql-lock-dml.png)

4. **如何安全地给小表加字段**

   - 首先我们要解决长事务, 事务不提交, 就会一直占着 MDL 锁
   - 在 MySQL 的 information_schema 库的 innodb_trx 表中可以查到当前执行中的事务
   - 如果你要做 DDL 变更的表刚好有长事务在执行, 要考虑先暂停 DDL, 或者 kill 掉这个长事务
   - 但如果要变更的表是一个热点表, 虽然数据量不大, 但是上面的请求很频繁

     - 这时候 kill 可能未必管用, 因为新的请求马上就来了
     - 比较理想的机制是，在 alter table 语句里面设定等待时间, 如果在这个指定的等待时间里面能够拿到 MDL 写锁最好, 拿不到也不要阻塞后面的业务语句, 先放弃
     - 之后开发人员或者 DBA 再通过重试命令重复这个过程

     ```sql
     -- MariaDB 和 AliSQL 有这样的功能
     ALTER TABLE tbl_name NOWAIT add column ...
     ALTER TABLE tbl_name WAIT N add column ...
     ```

### 18.行锁

1. 两阶段锁协议: 行锁是在需要的时候才加上, 但并不是不需要了就立刻释放, 而是要等到事务结束时才释放
2. **如果你的事务中需要锁多个行, 要把最可能造成锁冲突, 最可能影响并发度的锁尽量往后放**
   - 顾客 A 要在影院 B 购买电影票: `从顾客A账户余额中扣除电影票价 + 给影院B的账户余额增加这张电影票价 + 记录一条交易日志`
   - 如果同时有另外一个顾客 C 要在影院 B 买票，那么这两个事务冲突的部分就是语句 2 了, 按照 3、1、2 这样的顺序提升了并发度
3. CPU 消耗接近 100%, 整个数据库每秒就执行不到 100 个事务, 是由 死锁和死锁检测 到最后的
4. 死锁: `并发系统`中`不同线程`出现`循环资源依赖`, 涉及的线程都在`等待`别的线程`释放资源`时，就会导致这几个线程都进入`无限等待`的状态，称为**死锁**

   ![avatar](/static/image/db/mysql-dead-lock.webp)

   - 直接进入等待, 直到超时: 参数 **innodb_lock_wait_timeout** 来设置 50s
   - 发起死锁检测, 发现死锁后, 主动回滚死锁链条中的某一个事务, 让其他事务得以继续执行: 参数 **innodb_deadlock_detect** 设置为 on
   - 1000 个并发同时更新同一行, 死锁检测操作就是 100 万这个量级`[看它所依赖的线程有没有被别人锁住, 最后判断是否出现了循环等待]`
   - 死锁 CPU 消耗: 临时把死锁检测: 出现超时是业务有损的
   - 死锁 CPU 消耗: 控制并发度[服务端{相同行的更新在进入引擎之前排队}]
   - 死锁 CPU 消耗: 将一行改成逻辑上的多行来减少锁冲突

### 19.mvvc

1. 用于支持 RC 和 RR 隔离级别的实现
2. 事务在启动的时候就 `拍了个快照`: 基于整库的, 并不是物理上的快照, 是基于数据 row 的唯一的`事务ID{严格递增}`
3. 数据表中的一行记录, 其实可能有多个版本(row), 每个版本有自己的 row trx_id, 同时旧的数据版本可以根据新版本 + undo_log 得到
4. V1、V2、V3 并不是物理上真实存在的，而是每次需要的时候根据当前版本和 undo log 计算出来的
   ![avatar](/static/image/db/mysql-mvcc.png)

5. 一个事务在启动时声明: 以我启动的时刻为准, 如果一个数据版本是在我启动之前生成的则可见; 如果是我启动以后才生成的则不可见, 必须要找到它的上一个版本

   - 只要修改就会产生新的版本快照: 最新的版本快照就会变
     ![avatar](/static/image/db/mysql-mvcc-flow.png)

6. 更新数据都是先读后写的, 只能读当前已提交的值, 称为 "当前读"[current read], 避免之前的写丢失
7. `查找创建版本⼩于或等于当前事务版本, ~~删除版本为空或者⼤于当前事务版本~~`

### 20.唯一索引的使用 & 普通索引

1. 查询过程

   - 在索引树上查找, B+树从树根开始, 按层搜索到叶子节点: `数据页内部通过二分法来定位记录`
   - 普通索引: 查找到满足条件的第一个记录后, 需要查找下一个记录直到碰到第一个不满足 k=5 条件的记录
   - 唯一索引: 索引定义了唯一性, 查找到第一个满足条件的记录后, 就会停止继续检索
   - **但是两者之间几乎没有太大的性能差距**:
     1. InnoDB 的数据是按数据页为单位来读写的: `默认是16KB{一个数据页可以放近千个long的key}`
     2. 当需要读一条记录的时候, 并不是将这个记录本身从磁盘读出来, 而是以页为单位, 将其整体读入内存
     3. 所以都是读取一次数据页到内存, 之后的查找判断是很快的: 除非有大于 16K 的数据才会多次加载数据页

2. 更新过程: **change buffer**{插入一个新记录(4,400)}

   - 该记录对应的**目标页**在内存中
     1. 对于唯一索引来说, 找到 3 和 5 之间的位置, 判断到没有冲突, 插入这个值, 语句执行结束: redo-log + bin-log
     2. 对于普通索引来说, 找到 3 和 5 之间的位置, 插入这个值, 语句执行结束: redo-log + bin-log
   - 该记录对应的**目标页**不在内存中
     1. 对于唯一索引来说, **需要将数据页读入内存{随机读}**, 判断到没有冲突, 插入这个值, 语句执行结束: redo-log + bin-log
     2. 对于普通索引来说, 则是将更新记录在 change buffer{可以新开辟}, 语句执行就结束: **redo-log{记录的是插入一行}** + bin-log
   - 之后的查找
     ![avatar](/static/image/db/mysql-change-buffer-read.png)

3. 唯一索引可以保证业务的数据唯一性
   - **可以转换为逻辑判断**{做不到则就只能使用唯一索引}
   - 但是会导致 内存命中率{更新操作} 下降
   - "归档库"的场景, 可以考虑使用唯一索引的

### 21.change buffer: `可以持久化的数据` + `只有普通索引可以使用`

1. 当需要更新一个数据页时, 如果数据页在内存中就直接更新: 之后就是 redo_log, bin_log 的二阶段提交

2. 当不在内存中的话, 在**不影响数据一致性的前提**下, InooDB 会将更新操作缓存在 change buffer 中, 这样就不需要从磁盘中读入这个数据页
3. 查询时需要访问这个数据页的时候: 将数据页读入内存, 然后执行 change buffer 中与这个页有关的操作 `[merge]`
4. **merge: 将 change buffer 中的操作应用到原数据页, 得到最新结果**
5. merge 时机: 访问该数据页, 系统后台线程定期 merge, 数据库正常关闭
6. change buffer 的好处:

   - 将更新操作先记录在 change buffer, 减少读磁盘, 语句的执行速度会得到明显的提升
   - 数据读入内存是需要占用 buffer pool 的, 所以这种方式还能够避免占用内存, 提高内存利用率

7. 唯一索引的更新操作都要先判断这个操作是否违反唯一性约束: 就需要将数据也读取到内存判断, 所以不能使用 change buffer
8. change buffer 用的是 buffer pool 里的内存: innodb_change_buffer_max_size 占 bp 的比例
9. 适合的场景: 写多读少, 比如账单类、日志类的系统
10. 写入之后马上会做查询:
    - 将更新先记录在 change buffer
    - 但之后由于马上要访问这个数据页, 会立即触发 merge 过程
    - 这样随机访问 IO 的次数不会减少`{读取到内存才能更新}`, 反而增加了 change buffer 的维护代价
11. redo log & change buffer

    - redo log 主要节省的是随机写磁盘的 IO 消耗[转成顺序写]
    - 而 change buffer 主要节省的则是随机读磁盘的 IO 消耗

---

## mysql 参数

1. innodb_buffer_pool_size: `物理内存 * 3/4`, cache table and index data.

---

## question list

1. CPU 一直 100%

   - show processlist: 有一个一直再执行, 临时表一直在创建, CPU 一直很高
   - kill 不掉{只发出指令}, 是个 bug 就直接重启数据库

   ```sql
   show processlist ;
   select  * from information_schema.INNODB_TRX;
   kill query  263607;

   show global status like '%created_tmp%';
   show variables like '%table_size%';
   ```

2. [Innodb 缓冲池命中率计算](https://hdm.console.aliyun.com/dbMonitor/MySQL#/performance/instance/rm-2zefr83ok57101uxm/detail)

   ```sql
   show global status like 'innodb%read%';
   ```

   - Innodb_buffer_pool_reads: 表示从物理磁盘读取页的次数
   - Innodb_buffer_pool_read_ahead: 预读的次数
   - Innodb_buffer_pool_read_ahead_evicted: 预读的页, 但是没有读取就从缓冲池中被替换的页的数量, 一般用来判断预读的效率
   - Innodb_buffer_pool_read_requests: 从缓冲池中读取页的次数
   - Innodb_data_read: 总共读入的字节数
   - Innodb_data_reads: 发起读取请求的次数，每次读取可能需要读取多个页

   - **Innodb 缓冲池命中率计算: (Innodb_buffer_pool_read_requests - Innodb_buffer_pool_reads) / Innodb_buffer_pool_read_requests**
   - InnoDB Buffer Pool 使用率: **innodb_buffer_pool_pages_data / (innodb_buffer_pool_pages_data + innodb_buffer_pool_pages_free)**

3. 读取索引页和数据页的数据量

   ```sql
   select index_name, count(*)
   from information_schema.INNODB_BUFFER_PAGE
   where INDEX_NAME in('idx_recordid', 'primary') and  TABLE_NAME='all_star_online_pk_record_detail'
   group by index_name;
   ```
