## 关键字执行过程

### in/exist

1. IN 扫描全表, in +自查询会使用到临时表存储
2. EXISTS 只要查到一行数据满足条件就会终止查询, 不会有临时表
3. 大小表问题 + 关联子查询问题

   ```sql
   -- b should be less, 慢: 临时表+表扫描
   select *
   from class_a
   where id in (select id from  class_b);

   -- a should be less, 快: 没有临时表+满足一个就保留a+可能使用上索引
   select *
   from class_a a
   where exists (
      select id
      from class_b  b
      where a.id = b.id
   );

   -- best
   select *
   from class_a a
   inner join class_b b on a.id = b.id
   ```

4. 多个 in

   ```sql
   -- 两个子查询, 也就产生了两个中间表
   select id, state, city
   from addresses1 a1
   where state in (select state
                   from addresses2 a2
                   where a1.id = a2.id)
    and city in (select city
                 from addresses2 a2
                 where a1.id = a2.id);

   -- 没有中间表产生, 而且只执行一次即可
   select *
   from all_star_single_sign_up
   where id || member_id || updated_time
   in (select id || member_id || updated_time
         from all_star_single_sign_up );
   ```

### count

1. 简介

   - `count(*)` 推荐
   - count(1): 不要使用, 使用 `count(*)`
   - count(feild): 获取 field 不为空的个数

2. `count(*/1)` MySQL 都会用成本最小的辅助索引{非聚簇索引}查询方式来计数

### or

1. mysql 5.0 之前会导致索引失效
2. 之后的版本中引入了索引合并: 改善了一点效率(type: index_merge)

   - 把多条件查询(比如 or 或 and)的条件对多个索引分别进行条件扫描, 然后将它们各自的结果进行合并, 因此就可能不会导致索引失效的问题

3. exp

   ```sql
   -- idx_name + id is pri
   -- type: index_merge, extra: Using union(idx_name,PRIMARY); Using where
   explain select * from account_member where nickname = 'zack' or id=9;

   -- 此时 t || id || nickname 只要有一个没索引就会全表扫描
   explain select * from account_member where nickname = 'zack' or id=9 or t=xx;
   ```

### order by

0. 想获取到排好序的结果:

   - 对结果集进行排序的操作: 获取数据之后排序
   - 按照索引顺序扫描得出的结果自然是有序的: 在有序的数据上截取

1. overview: 2 个参数 + 1 建议 + 2 个特殊 + 正排倒排 + 1 注意

   - [一般不要改]sort_buffer_size: 确定内存中排, 磁盘上外排{归并排序}
   - [一般不要改]max_length_for_sort_data(1024): 确定是 全字段排序 || Rowid 排序
   - 1 建议: 善于使用副高索引
   - 2 个特殊: 无条件排序不会走索引{非覆盖索引} + **limit** 可能优先队列排序{filesort_priority_queue_optimization }
   - 正排倒排: 8.x
   - 1 注意: Using filesort 在高并发下一定会有问题

2. pre-env

   ```sql
   create table `staff` (
      `id` bigint ( 11 ) auto_increment comment '主键id',
      `id_card` varchar ( 20 ) not null comment '身份证号码',
      `name` varchar ( 64 ) not null comment '姓名',
      `age` int ( 4 ) not null comment '年龄',
      `city` varchar ( 64 ) not null comment '城市',
      primary key ( `id`),
      index idx_city ( `city` )
   ) engine = innodb comment '员工表';

   select name, age,city from staff where city = '深圳' order by age limit 10;
   ```

   ![avatar](/static/image/db/mysql-key-orderby-data.png)

3. 全字段排序: 需要的全部字段都放到 sort_buffer 中, 排序后就会直接从内存里面返回查询结果

   - sql flow

     1. MySQL 为对应的线程初始化 sort_buffer, 放入需要查询的 **name、age、city** 字段
     2. 从索引树 idx_city, 找到第一个满足 city='深圳’条件的主键 id, 也就是图中的 id=9
     3. 到主键 id 索引树拿到 id=9 的这一行数据, 取 name、age、city 三个字段的值, 存到 sort_buffer
     4. 从索引树 idx_city 拿到下一个记录的主键 id, 即图中的 id=13
     5. 重复步骤 3、4 直到 city 的值不等于深圳为止
     6. 前面 5 步已经查找到了所有 city 为深圳的数据, 在 sort_buffer 中, 将所有数据根据 age 进行排序
     7. 按照排序结果取前 10 行返回给客户端。

     ![avatar](/static/image/db/mysql-key-orderby-mem-flow.png)

   - 过程中 sort_buffer 如果满了则会使用 `磁盘临时文件辅助排序`: **optimzier_trace's number_of_tmp_files**
     ![avatar](/static/image/db/mysql-key-orderby-optra-disk.png)

     1. 从主键 Id 索引树, 拿到需要的**需要的全部**, 并放到 sort_buffer 内存块中: **`这里的字段可以控制(rowid 排序)`**
     2. 当 sort_buffer 快要满时, **就对 sort_buffer 中的数据排序**, 排完后, 把数据临时放到磁盘一个小文件中
     3. 继续回到主键 id 索引树取数据, 继续放到 sort_buffer 内存中, 排序后, 也把这些数据写入到磁盘临时小文件中
     4. 继续循环, 直到取出所有满足条件的数据
     5. 最后把磁盘的临时排好序的小文件, 合并成一个有序的大文件: 归并排序算法

   - 既然 sort_buffer 放不下, 就需要用到临时磁盘文件{影响排序效率}: 只放排序相关的 age 字段进 bubffer, 不相关的字段（name, city）==> `rowid 排序`

4. Rowid 排序: 内存放 rowid 与排序字段{`排序的字段+主键id`}, 排序后, 再从库中找数据, 拼接返回

   - max_length_for_sort_data: 默认 1024 字节(1k), 需要的字段占用字节数的和 <= 1024 就不会启用 rowid 排序{**因为 mysql 认为数据不大, 却可以减少回表**}

     - **`使用的是字段的定义最大长度, 而不是实际存储的长度`**
       ![avatar](/static/image/db/mysql-key-orderby-optra-rowid.png)

   - sql flow

     1. MySQL 为对应的线程初始化 sort_buffer, 放入需要排序的 age 字段, 以及主键 id
     2. 从索引树 idx_city, 找到第一个满足 city='深圳' 条件的主键 id, 也就是图中的 id=9
     3. 到主键 id 索引树拿到 id=9 的这一行数据, **取 age 和主键 id 的值**, 存到 sort_buffer
     4. 从索引树 idx_city 拿到下一个记录的主键 id, 即图中的 id=13
     5. 重复步骤 3、4 直到 city 的值不等于深圳为止
     6. 前面 5 步已经查找到了所有 city 为深圳的数据, 在 sort_buffer 中, 将**所有数据根据 age 进行排序**
     7. 遍历排序结果, 取前 10 行, 并按照 id 的值回到原表中, 取出 city、name 和 age 三个字段返回给客户端

     ![avatar](/static/image/db/mysql-key-orderby-rowid.png)

5. 全字段排序 vs rowid 排序: max_length_for_sort_data

   - 全字段排序: sort_buffer 内存不够的话, 就需要用到**磁盘临时文件**, 造成磁盘访问
   - rowid 排序: sort_buffer 可以放更多数据, 但是需要再回到原表去取数据, 比全字段排序**多一次回表**

6. 倒排

   - 数据页页中有一块空间叫"槽": **槽中存放着每个分组内最后一条记录在页面中的地址偏移量**

7. 5.6 之后, 排序字段来自不同表的话, **会将关联结果保存到临时表中**: using temporary;using filesort
8. 5.6 之后, 针对 order by limit 做了个小优化:

   - **排序字段无索引且列值不唯一时**: 优化器在遇到 order by limit 语句的时候使用了 priority queue

9. 优化建议

   - sort_buffer_size + max_length_for_sort_data
   - 无 where 基本上都不走索引{**mysql 认为全表的回表代价大**}: limit || 加条件
   - limit 大页问题
   - in 多个值会导致断层的索引失效{1 个不会}: 可以拆开写 SQL 之后再~~归并~~排序
   - 联合索引{正排倒排} + 覆盖索引: 索引存储顺序与 order by 不一致

     ```sql
     alter table staff add  index idx_city_age(city,age);

     -- sql flow
     -- 1. 从索引idx_city_age找到满足city='深圳’ 的主键 id
     -- 2. 到主键 id索引取出整行, 拿到 name、city、age 三个字段的值, 作为结果集的一部分直接返回
     -- 3. 从索引idx_city_age取下一个记录主键id
     -- 4. 重复步骤 2、3, 直到查到第10条记录, 或者是不满足city='深圳' 条件时循环结束
     ```

     ![avatar](/static/image/db/mysql-key-orderby-opt-flowr.png)

### limit 大数

1. 原因

   - limit 100000, 20 的意思扫描满足条件的 100020 行(回表的){**可以查看 sql 的读取数据页的数量**}, 扔掉前面的 100000 行, 返回最后的 20 行
   - 大量的时间都花费在随机 IO[回表]上: 尤其查询偏移大于**10 万**以后查询时间急剧增加

   ```sql
   -- limit 大数: 慢 & 污染buffer pool
   select index_name,count(*)
   from information_schema.INNODB_BUFFER_PAGE
   where INDEX_NAME in('val','primary') and TABLE_NAME like '%test%'
   group by index_name;
   ```

2. 解决方案: `覆盖索引-数据ID`

   - limit 默认会加载所有数据并过滤掉前面的 10w 行, 只取 20 行: 不是很好
   - solution01: 使用条件过滤{将 limit 转换为过滤条件}
   - solution02: **利用覆盖索引, 省去回表的耗时: 写子查询去确定边界**
   - 也可以在业务允许的情况下, 限制页数

3. sample

   ```sql
   -- 慢 & 加载了很多热点不是很高的数据页到buffer pool, 会造成buffer pool的污染, 占用buffer pool的空间
   SELECT id, title, ico, create_time, create_user
   FROM article_copy1
   where `status` = '1'
   ORDER BY id DES LIMIT 100000, 10

   -- solution01: 将上一次的最大的id传递给前端
   SELECT id, title, ico, create_time, create_user
   FROM article_copy1
   WHERE `status` = '1' and id < 2155652
   ORDER BY id DESC
   LIMIT 10;

   SELECT id, title, ico, create_time, create_user
   FROM article_copy1
   WHERE `status` = '1' and id >= (SELECT id FROM article_copy1  WHERE `status` = '1' ORDER BY id DESC LIMIT 100000, 1)
   ORDER BY id DESC
   LIMIT 10


   SELECT id, title, ico, create_time, create_user
   FROM article_copy1 c1
   join (SELECT id FROM article_copy1  WHERE `status` = '1' ORDER BY id DESC LIMIT 100000, 1) t on t.id=c1.id
   WHERE `status` = '1'
   ORDER BY id DESC
   LIMIT 10
   ```

### order by 无索引且有重复列, 使用 limit 会和预期不一样

1. pre env

   ```sql
   create table `ratings` (
      `id` int(11) not null auto_increment,
      `category` int(11) default null,
      primary key (`id`)
   ) engine=innodb auto_increment=11 default charset=utf8mb4 collate=utf8mb4_general_ci;

   insert into ratings (id, category) values (1, 1);
   insert into ratings (id, category) values (5, 1);
   insert into ratings (id, category) values (10, 1);
   insert into ratings (id, category) values (3, 2);
   insert into ratings (id, category) values (4, 2);
   insert into ratings (id, category) values (6, 2);
   insert into ratings (id, category) values (9, 2);
   insert into ratings (id, category) values (2, 3);
   insert into ratings (id, category) values (7, 3);
   insert into ratings (id, category) values (8, 3);
   ```

2. 执行的 SQL

   ```sql
   -- 实际: 1, 10, 5, 3, 4
   -- 预期: 1 5 10 3 4
   select * from ratings order by category limit 5;
   ```

3. 原因: 5.6 之后, 针对 order by limit 做了个小优化:

   - **排序字段无索引且列值不唯一时**: 优化器在遇到 order by limit 语句的时候使用了 priority queue
   - sort_buffer_size 小于 limit 数量的需要的西段(sort_key+rowid/`select 的字段`): 否则还是会使用 归并排序
   - 验证是否使用优先队列: `"filesort_priority_queue_optimization": {}`

4. 解决

   - 最优解: order by category,id

   ```sql
    -- 实际: 1, 10, 5, 3, 4
    -- 预期: 1 5 10 3 4
    select * from ratings order by category,id limit 5;
   ```

   - category 加索引: category 区分度不好

### group by

### join

1. 子查询
2. join 原理: a join b --> join c

   - left join: 如果可以命中索引, 就会先使用 索引 过滤一下数据, 之后才会进行 join;
   - left join: 如果没有则会先做 join 之后根据 where 对 join 之后的结果进行过滤

3. 关联的算法是 Nest Loop Join:

   - 是通过驱动表的结果集作为循环基础数据, 然后一条一条地通过该结果集中的数据作为过滤条件到下一个表中查询数据, 然后合并结果
   - 如果还有第三个参与 Join, 则再通过前两个表的 Join 结果集作为循环基础数据, 再一次通过循环查询条件到第三个表中查询数据, 如此往复
   - 所以, 小表驱动大表所建立的连接次数也远比大表驱动小表所建立的连接次数要小的多 + **被被驱动表上简历索引**
   - inner join mysql 会自动优化大小表的顺序
   - 多关联一个表, 就会多分配一个关联缓存(join_buffer_size): 如果在一个 SQL 中关联的表越多, 所占用的内存也就越大
   - join_buffer_size 参数不合理: 容易造成服务器内存溢出的情况, 影响数据库性能的稳定性

4. 小表在前的原理:

   - 最终产生的 row 不会变化, 但是过程中时不一样的 cost
   - 左表就一条记录, join 右表则是顺序读

5. a left join b ?? b left join a

   - 如果 where 条件中含有 table_b 的非空条件[除开 is null] 会被优化成笛卡尔积的 join, 否则按照 SQL 执行
   - 优化器回去比较这两种的 cost: `scan a' cost[io+cpu] + a's rows[某个地方记录的近似值] * b's cost`

6. calculate a's cost between all-scan and index: io+cpu

   - 全表扫描的 cost: rows \* 读取一行的 cost + 顺序读
   - 走索引的 cost: 走索引的 cost 会默认认为回表 索引, 扫描该索引的 cost + 索引过滤后的行数 \* 回表时读取一行的 cost + 随机读

7. table_a has index of idx_a and idx_b, how to choose index

   - idx_a's cost: 扫描 idx_a 的 cost + idx_a 过滤后的行数 \* 回表时读取一行的 cost
   - idx_b's cost: 扫描 idx_b 的 cost + idx_b 过滤后的行数 \* 回表时读取一行的 cost

### rand(): 随机下的性能

- sort_buffer_size

```sql
-- 1w rows
explain select word from t_words order by rand() limit 3; {Using temporary; Using filesort}
```

1. order by rand() 执行过程: 扫描行数很多有性能有问题的

   - 创建一个临时表{内存/磁盘}: memory 引擎的{**tmp_table_size** 值大于会被放入的值}, R(), W{字段长度}
   - 从 words 表中, 按主键顺序取出所有的 word 值, 插入临时表, **扫描 1w 行**
   - 现在临时表有 10000 行数据, 按照字段 R 排序
   - 初始化 sort_buffer{double 类型, 整型}: sort_buffer_size 会影响排序算法{归并排序算法[临时文件], 优先队列排序算法}
   - 临时表中一行一行地取出 R 值和位置信息存入 sort_buffer[对内存临时表做全表扫描], **扫描 +1w 行**
   - 在 sort_buffer 中根据 R 的值进行排序
   - 排序完成后, 取出前三个结果的位置信息, 依次到内存临时表中取出 word 值, 返回给客户端, **扫描 +3 行**
   - 这个过程中, 总扫描行数变成了 20003

   ![avatar](/static/image/db/mysql-random.png)

2. **order by rand()使用了内存临时表, 内存临时表排序的时候使用了 rowid 排序方法{优先队列排序算法}**

   - 内存临时表: 这个适合参数 tmp_table_size 有关
   - rowid: 这个是因为内存临时表, 不会回表问题
   - 优先队列排序算法: sort_buffer_size 大于 limit{真实需要的字段大小{这里就是 rowid+R} \* limit}

3. 随机排序方法

   - `M=max(id), N=min(id), X = (M-N)*rand() + N` + 取不小于 X 的第一个 ID 的行

     1. 这个会使用索引, 不会大量扫描数据
     2. **但是并不是真正的随机**
     3. code

     ```sql
     select max(id),min(id) into @M,@N from t ;
     set @X= floor((@M-@N+1)*rand() + @N);
     select * from t where id >= @X limit 1;
     ```

   - **`C=count(*) + Y, floor(C * rand()) , limit Y,1`**

     1. 一共会扫描 C+Y+1 行: count(\*) 扫描 C 行 + `limit Y,1` 会扫描 Y+1 行
     2. 但是由于是 id primary, 代价比 order by random() 小很多
     3. **解决了上一个方案中的不是真正随机问题**: 与 Id 无关, 以个数为基准的随机

     ```sql
     -- select * from t limit count(*)*rand() 1;
     mysql>
        select count(*) into @C from t;
        set @Y1 = floor(@C * rand());
        set @Y2 = floor(@C * rand());
        select * from t limit @Y1, 1; //在应用代码里面取Y1、Y2值，拼出SQL后执行
        select * from t limit @Y2, 1;
     ```

   - 再优化: C + Y2 + 1

     ```sql
     -- select * from t limit count(*)*rand() 1;
     mysql>
        select count(*) into @C from t;
        set @Y1 = floor(@C * rand());
        set @Y2 = floor(@C * rand());
        -- 比大小设置 @Y1 小于 @Y2
        id1 = select * from t limit @Y1, 1;
        select * from t where id > id1 limit @Y2 - @Y1, 1;
     ```
