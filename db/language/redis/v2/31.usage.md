## 0. 使用场景: 遇事不决就 hash

1. [list]: [空间上差不多都是 ziplist] 只能替换某一条, hash 是可以修改大对象都是 ziplist 的某个属性
   - stack/queue/array/bqueue
2. [**hash**] 存放复杂对象 + 配置文件列表
3. [string]分布式锁
4. [string]限流控制: 漏桶 / 令牌 / 计数 / 拥塞窗口
5. [string]幂等控制
6. [string]预减库存/点赞
7. [string]缓存用戶信息: 登錄, 这里主要泛型擦除[java]
8. [string-bitmap]布隆过滤器
9. [string-bitmap]签到: bitmap + string
10. [string] 日活
11. [hyperloglog]uv 统计: hyperloglog
12. [zset]排行榜: 多字段排序 + hash 做的记录那个分数[因为涉及负载计算+rpc], `k(storeid) + v(object{usrid + score})`
13. [zet]_抽奖: 社区 bug 抽奖 + set 可以做, 但是最终还是使用了 mysql 在内存中做的_
14. [set]好友 set: 共同關注 + 粉絲集合 + 可能感興趣
15. [string/hash]应对并发: 只保证原始数据的落库, 其他操作都在 redis 里, 之后定时任务同步会数据库 + 最终态的校验
16. lua 脚本批量删除: lua 脚本是原子性的, 所以每次连接只删除一点, 多次连接`获取连接与阻塞时长的平衡`

## 1. 排行榜的实现问题

1. 数据修改之后会存数据库操作
2. 但是会维护 redis 的排行榜: 避免每次都去查询数据库
3. redis command

   ```shell
   # 1. 设置玩家分数: o(log(n))
   # zadd 排行榜名称 分数 玩家标识
   zadd lb 89 user1
   zadd lb 95 user2
   zadd lb 95 user3

   # 2. 查看玩家分数: o(1)
   # zscore 排行榜名称 玩家标识
   zscore lb user2

   # 3. 按名次查看排行榜: o(log(n)+m)
   # zrevrange 排行榜名称 起始位置 结束位置 [withscores]
   zrevrange lb 0 -1 withscores

   # 4. 查看玩家的排名: o(log(n))
   # zrevrank 排行榜名称 玩家标识
   zrevrank lb user3 #0

   # 5. 增减玩家分数: o(log(n)), 没有的话默认为0
   # zincrby 排行榜名称 分数增量 玩家标识
   zincrby lb 6 user4

   # 6. zrem: o(log(n))
   # zrem 排行榜名称 玩家标识
   zrem lb user4

   # 7. 删除排行榜
   del lb
   ```

## 2. 签到系统

1. 需求

   - 显示用户某个月的签到次数和首次签到时间
   - 判断用户登陆状态: **一个大 key 就可以做完: 具体操作都是微操作** + 要配合用户的 id 系统一起
   - ~~两亿用户最近 7 天的签到情况, 统计 7 天内连续签到的用户总数~~: 报表数据可以走数据库[redis 是为了 toc 抗并发的] + redis 做的话 `key=yymmdd <userid> value`
   - 两亿签到: 数据量巨大使用 string 是不合理的{sds+redisobj 等都会占用空间} + bitmap 好且可以操作 bit

2. 常用的统计模式

   - 二值状态统计: bitmap
   - 聚合统计[交并差补]: set
   - 排序统计: list/zset
   - 基数统计: hyperloglog

3. redis bitmap: bitmap 是基於 string 類型的按位與操作 + 最大位 2^32=512m

   - `一個人一月一個 key: upms:member:1:202001 0 1` || ~~`key=yymmdd <userid> value`~~
   - 一人一月: 31 天 4byte
   - 一月: 4byte \* 1 千萬用戶 / 1024 /1024 = 38m
   - 一年: 38m \* 12 = 456m

4. redis command

   ```shell
   # 第一天簽到
   aliyun:14>setbit upms:member:sign:1:202108 0 1
   "0"
   # 第二天簽到
   aliyun:14>setbit upms:member:sign:1:202108 1 1
   "0"
   # 第11天簽到
   aliyun:14>setbit upms:member:sign:1:202108 10 1
   "0"
   # 第12天簽到
   aliyun:14>setbit upms:member:sign:1:202108 11 1
   "0"
   # 第14天簽到
   aliyun:14>setbit upms:member:sign:1:202108 13 1
   "0"
   # 第15天簽到
   aliyun:14>setbit upms:member:sign:1:202108 14 1
   "0"

   # 查看第15天簽到情況
   aliyun:14>getbit upms:member:sign:1:202108 14
   "1"
   # 查看第11天簽到情況
   aliyun:14>getbit upms:member:sign:1:202108 10
   "1"
   # 查看第13天簽到情況
   aliyun:14>getbit upms:member:sign:1:202108 12
   "0"
   # 查看202108月簽到次數
   aliyun:14>bitcount upms:member:sign:1:202108
   "6"

   # 查看202108月第一次簽到是第幾天
   aliyun:14>bitpos upms:member:sign:1:202108 1
   "0"
   # 查看202108月第一次沒簽到是第幾天
   aliyun:14>bitpos upms:member:sign:1:202108 0
   "2"

   # 查看202108月1-4號的簽到情況: 最終得到的是這些位上的數字轉long: u表示按照無符號轉換
   # 从 0 号位置开始取3位
   aliyun:14>bitfield upms:member:sign:1:202108 get u3 0
   1) "6"     110          表示第一天登录. 表示第二天登录, 表示第三天没登录.
   # aliyun:14>setbit upms:member:sign:1:202108 3 1
   # aliyun:14>bitfield upms:member:sign:1:202108 get u4 0
   # 1) "13"     1101  表示第一天登录. 表示第二天登录, 表示第三天没登录. 表示第四天登录
   aliyun:14>bitfield upms:member:sign:1:202108 get u4 0
   1) "12"
   # 查看202108月1-4號的簽到情況: 最終得到的是這些位上的數字轉long: i示按照有符號轉換
   aliyun:14>bitfield upms:member:sign:1:202108 get i3 0
   1) "-2"
   aliyun:14>bitfield upms:member:sign:1:202108 get i4 0
   1) "-4"
   ```

## 3. 日活: set/hash/bitmap/hyperloglog

1. set 但是过于占用空间: `sadd + scard `
2. hash: hash 的 key 也有去重功能 + `hset+hlen`
3. bitmap[本质是 string]: 最大的用户 id 是 512m, 2^32=42 亿 + `getbit/setbit/bitcount`
4. 如果数据 id 不是连续的获取数据量不大就不合适
5. redis command

   ```shell
   # key=xxyymmdd + offset(uid)
   # 用户登录
   setbit login1111 uid-1 1
   setbit login1111 uid-2 1
   setbit login1111 uid-3 1
   setbit login1111 uid-4 1

   # strlen login1111  可以查看空间的占用
   bitcount login1111 0 1 # 注意这里的 0-1 指的是存储空间[len]
   # 查看日活
   bitcount login1111

   # 12 号的用户登录信息
   setbit login1112 uid-1 1
   setbit login1112 uid-4 1
   setbit login1112 uid-5 1

   # 连续两天都登录的人
   bitop and login1111-and-login1112  login1111 login1112
   # 两天一共登录的人
   bitop or login1111-or-login1112  login1111 login1112
   ```

## 4.分布式锁

- 分布式锁: **控制分布式系统不同进程共同访问共享资源的一种锁的实现**, 如果不同的系统或同一个系统的不同主机之间共享了某个临界资源, 往往需要互斥来**防止彼此干扰**, 以保证一致性

### 本地缓存锁

![avatar](/static/image/db/redis-standalone.png)

1. 单机版在多线程下需要加锁
2. 确保以下 3 个动作是原子的, 二期是在锁住的方法内
   - query from redis cache
   - query database
   - put result to redis cache

### 分布式缓存锁: mysql(悲观锁/乐观锁[cas-version]) || redlock || redis || zk

![avatar](/static/image/dist/dist-lock.png)
![avatar](/static/image/db/redis-lock-overview.png)

1. 集群微服务: 单机版锁值管自己的 jvm 层面的, 可能会导致 2 个服务买同一个商品
2. core: 在指定地方占一个位置, 占上位置就可以执行逻辑, 否则等待
   ![avatar](/static/image/db/redis-distribute.png)
3. 分布式锁的注意点

   - 加锁的原子性: `set k v nx ex`
   - 解**自己**锁的原子性: lua 或者 ~~redis 事务: watch + transaction+ multi+delete + unwatch~~
     ```lua
     // 返回值是 long
     if redis.call('get',keys[1]) == argv[1] then
       return redis.call('del',keys[1])
     else return 0
     end
     ```
   - 解/删除锁代码的一定执行 + ex
   - 解决业务超时问题锁续期问题 + 只能删除自己的锁
   - redis 集群时异同同步数据导致的 set 丢失问题: 自己手动修复数据

4. evolution

   ![avatar](/static/image/db/redis-distribute-v1.png)
   ![avatar](/static/image/db/redis-distribute-v2.png)
   ![avatar](/static/image/db/redis-distribute-v3.png)
   ![avatar](/static/image/db/redis-distribute-v4.png)

5. redission 也还是会出现主从同步锁丢失的问题:

   - zookeeper 可以解决这个问题(cp)
   - 也可以使用 redlock 向多个 redis 内加锁, 半数成功才认为加锁成功

6. java 代码中 stw 会有续期的问题

   - zk 临时节点在 stw 时没有心跳
   - redis 也是的

7. 继续优化

   - 分段锁: 将库存分为多个阶段, 对每个阶段进行加锁减库存操作
   - 读写锁

8. [redlock](https://mp.weixin.qq.com/s/rvidm1whe61sdlnkzumtag): 一般没人会用

   - 为了解决主从架构下的锁丢失问题: 发生的情况就很少
   - 官方建议在不同机器上部署 5 个 redis 主节点: 5-单数个[节点容错], 节点独立对等, 没有主从
   - flow: 客户端要获取锁有 5 个步骤

     1. 客户端获取当前时间 t1[毫秒级别]
     2. 使用相同的 key 和 value 顺序尝试从 n 个 redis 实例上获取锁: 每个请求都有毫秒级别的超时时间[快速向下一个实例发送请求]
     3. 客户端获取当前时间 t2 并减去 t1 来计算出获取锁所用的时间 t3 = t2 -t1:
        - 当且仅当客户端在大多数实例[n/2 + 1]获取成功, 且获取锁所用的总时间 t3 小于锁的有效时间, 才认为加锁成功, 否则加锁失败
     4. 如果第 3 步加锁成功, 则执行业务逻辑操作共享资源: key 的真正有效时间等于有效时间减去获取锁所使用的时间(t3)
     5. 如果获取锁失败[没有 n/2+1 个 server/取锁时间已经超过了有效时间], 客户端应该在所有的 redis 实例上进行解锁[即便某些 redis 实例根本就没有加锁成功]

   - 缺点: martin 分布式大佬`{分布式有三大难题 npc(network delay(网络延迟)/process pause(进程暂停[gc])/clock drift(时钟漂移)}`

     1. 锁定的目的是为了保护对共享资源的读写, **分布式锁应该「高效{避免不必要地做同样的两次工作}」和「正确{使用锁用来防止并发进程互相干扰}」**: 5 台 server 复杂且不高效 + 只有一个线程能对共享数据读写
     2. `5 台 server 很重 === redlock 作者说可以解决网络延迟、进程暂停, 存在锁失效的问题(gc时)`
     3. `默认自带假设: 多个节点机器时钟都是一致 === redlock 作者说大概一致就行[现在的 server 基本都能做到]`
     4. 无法保证正确性
     5. 请使用 zookeeper

     ![avatar](/static/image/db/redis-usage-redlock-diasdv.png)

9. zk(分布式协调) 分布式锁的过程

   - zookeeper 客户端会创建一个持久节点 locks
   - client1 想获得锁: 需要在 locks 节点下创建一个顺序的临时节点 lock1}{多个临时序列节点可以一次注册监听}
   - client1 会查找 locks 下面的所有临时顺序子节点, 判断自己的节点 lock1 是不是排序最小的那一个, 如果是, 则成功获得锁
   - client2 想获取锁: 查找 locks 下面的所有临时顺序子节点, 判断自己的节点 lock2 是不是最小的, 发现 lock1 才是最小的, 于是获取锁失败
   - client2 获取锁失败, 向它排序靠前的节点 lock1 注册 watcher 事件, 用来监听 lock1 是否存在, 也就是说 client2 抢锁失败进入等待状态
   - 如果再来一个客户端 client3 来尝试获取锁, 它会在 locks 下再创建一个临时节点 lock3: 获取锁失败 + 注册 lock2 的监听 + 进入获取锁等待
   - client1 任务完成会显式调用删除 lock1 的指令 || client1 故障, 根据临时节点得特性, lock1 是会自动删除的
   - lock1 节点被删除后: client2 一直监听着 lock1, lock1 节点删除, client2 立刻收到通知, 也会查找 locks 下面的所有临时顺序子节点, 发下 lock2 是最小, 就获得锁
   - 如果有很多的客户端频繁的申请加锁、释放锁, 对于 Zookeeper 集群的压力会比较大(有太多监听注册)

10. 分布式锁对比汇总

    - 性能角度: Redis(最好) > Zookeeper >= 数据库
    - 理解的难易程度角度:数据库(好理解) > Redis > Zookeeper
    - 实现的复杂性角度: Zookeeper(最难) > Redis > 数据库
    - 可靠性角度: Zookeeper(最可靠) > Redis > 数据库

## 5. 分布式缓存: 热点高频尽量不变的对象

1. 客户端缓存: 页面/浏览器缓存, app 缓存, h5 缓存, localstrage, session-storage
2. cdn 缓存: 内容存储, 数据存储, 内容分发[负载均衡]
3. nginx 缓存: 静态资源
4. 服务器缓存: 本地缓存, 外部缓存
5. 数据库缓存: 持久层缓存[mybatis 缓存], mysql 服务的缓存
6. 操作系统缓存: page cahce, buffer cache

### 缓存数据的一致性问题

![avatar](/static/image/db/redis-consistency-data.png)

1. 从设计思路来保证**数据一致性**{三种}

   - [旁路缓存模式]cache aside: **读请求比较多** + 把缓存责任交给应用层{从性能极致来说} + 同时维系 db 和 cache{并且是以 db 的结果为准}
   - [读写穿透]read/write through: 服务端把 cache 视为主要数据存储, 从中读取数据并将数据写入其中, cache 服务负责将此数据读取和写入 db, 从而减轻了应用程序的职责{很少使用(redis 不支持写 db)}

     1. 写入: 先查 cache, cache 中不存在, 直接更新 db; cache 中存在, 则先更新 cache, 然后 cache 服务自己更新 db[**同步更新** cache 和 db]
     2. 读取: 从 cache 中读取数据, 读取到就直接返回; 读取不到的话, 先从 db 加载, 写入到 cache 后返回响应
     3. cache(相当于 proxy) 服务自己来写入缓存的: 客户端是透明的

   - [异步缓存写入]write behind pattern: **由 cache 服务来负责 cache 和 db 的读写**(与读写穿透类似)

     1. read/write through 是同步更新 cache 和 db
     2. write behind caching 则是只更新缓存, 不直接更新 db, 而是改为异步批量的方式来更新 db
     3. 写性能非常高: 适合一些数据经常变化又对数据一致性要求没那么高的场景(如浏览量、点赞量)
     4. 应用: **mysql 的 innodb buffer pool 机制** || 消息队列中消息的异步写入磁盘

2. cache aside 解决方案

   - [失效模式]数据存到数据库中成功后, 再让缓存失效{lazy 计算的思想}: 等到读缓存不命中的时候再加载进去
   - [双写模式]先更新数据, 再更新缓存: 把 cache 也做 buffer 用
   - 通过消息队列更新缓存
   - [canal]起一个同步服务, 作为 mysql 一个从节点, 通过解析 binlog 同步重要缓存{超高并发或一致性要求高不适合 + **最终一致性的最优解**}

   ![avatar](/static/image/db/redis-consistency-update.png)

3. 双写模式: `第二个线程写缓存先执行了导致`的不一致, **而且缓存有时不是单纯的数据[经过相关的逻辑结果、频繁修改但是没有使用(更新缓存是没有意义的)]**

   ![avatar](/static/image/db/redis-data-consistency.png)

   - 可以串行化: 但是效率太低

4. **失效(可以使用 queue)模式**: 先写数据库, 之后再删除缓存{失败[自己封装的服务]-retry}

   - `第三个执行查询写入 redis 快于第二个, 会导致多查数据库`
   - `1 删除结束, 3 进行数据库查询, 2 更新了数据库并删除了缓存, 3 开始写如redis, 此时的数据就不会死最新的[2操作后的数据时最新的]`
   - **不一致概率非常小, 因为缓存的写入速度是比数据库的写入速度快很多**
   - 超高并发下不: **重试删除缓存就完全可以做成异步**
   - 存在的问题:

     1. 首次请求数据一定不存在 cache 的问题
     2. 写操作比较频繁的话导致 cache 中的数据会被频繁被删除, 会影响缓存命中率(鸵鸟 || 双写)

     ![avatar](/static/image/db/redis-data-consistency-v2.png)

   - solution1: 可以使用读写锁, 2 在写时, 3 读会被阻塞
   - solution2: 使用 blockingqueue 进行排序{更新数据的时候, 根据**数据的唯一标识**, 将操作路由之后, 发送到一个 jvm 内部队列中}
     1. 数据变更的操作: 先删除缓存, 然后再去更新数据库, 但是还没完成更新
     2. 此时读请求过来: 读到了空的缓存, 先将缓存更新的请求发送到队列中, 此时会在队列中积压, 然后同步等待缓存更新完成
     3. 一个队列中, 其实多个连续的更新缓存请求串在一起是没意义的: 可以过滤
     4. 缺点： 复杂 + 读请求长时阻塞 + 唯一数据的倾斜

5. 失效模式: 先删除缓存, 之后再写数据库

   - issue: 删除缓存失败则数据就不对了 & 删除缓存之后写入数据库之前有查询则缓存旧的数据
   - solution3: 延时双删[先删除缓存, 在写数据库, 之后休眠一会才删除缓存]

6. **延时双删**: 数据一致性有点点差

   - {线程池}异步~~串行化~~删除: ~~删除请求入队列~~
   - 休眠时间: 在读数据业务逻辑的耗时的基础上, 加上几百 ms 即可
   - 缓存删除失败: 重试机制, 可以借助消息队列的重试机制, 也可以自己整个表, 记录重试次数

7. 解决方案

   - 如果时用户维度的数据(订单数据, 用户数据), 并发几率小, `缓存+过期时间+失效模式` 就可以了
   - 如果时菜单, 商品介绍等数据[可以长时间数据不一致]: `canal-binlog`
   - 要求一致性高的可以考虑加锁: `读写锁`
   - 遇到实时性一致性要求超高的, 应该直接查询数据库

### 使用场景: `缓存只保证最终一致性`

0. 即使是这种单机应用, io 文件也不是每次都 fsync 存盘的 fsync 会带来性能的急剧下降
1. 即时性, 数据一致性要求不高: _物流信息_, _商品分类_
   - 缓存本来就有时差性(应该的), 如果是追求严格意义上的一致性则不要使用缓存
2. 访问量大, 但是更新频率不高的数据`[读多写少]`: _商品信息_
3. 但是还是要尽量减少 db 的操作: 删除缓存其实也不是太好的方式
4. **遇到实时性一致性要求超高的, 应该直接查询数据库**
5. 好的方案是使用 canal 或者使用封装好的服务[中台]

### 缓存分类

1. 本地缓存

   - 比如存在 map 中, 只适用于单体应用
   - 分布式下存在数据不一致问题

2. 分布式缓存

## 6. 分布式限流[拥塞窗口/令牌桶]

1. 在微服务架构下, 限频器也需要分布式化: 无论是哪种算法, 都可以结合 redis 来实现
2. 计数器限流: setnx
3. 拥塞窗口限流: zset{zresvrange}
4. 令牌桶限流: list + 定时任务
   - redis 负责管理令牌, 微服务需要进行函数操作, 就向 redis 申请令牌, 如果 redis 当前还有令牌, 就发放给它. 拿到令牌, 才能进行下一步操作
   - 另一方面, 令牌不光要消耗, 还需要补充, 出于性能考虑, 可以使用懒生成的方式(使用令牌时, 顺便生成令牌)
   - 好处: 令牌的获取, 和令牌的生成, 都可以在一个 lua 脚本中, 保证了原子性

## 7. 幂等方案

1. [link](/dist/01.common.md)
