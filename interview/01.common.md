## [接口幂等性](https://github.com/Alice52/java-ocean/issues/201)

1. 定义: 就是任意多次执行所产生的影响均与一次执行的影响相同
2. 场景

   - Feign 的 retry 机制
   - 用户多次点击
   - 重复提交[连续点了 2 次/web 端回退导致的]
   - 有一些操作用户退出了, 又重新点击进来: 放大了时间差

3. 操作本身具有幂等性

   - 带主键的插入,
   - 查询
   - 定量更新
   - 删除操作

4. 操作不具有幂等性的: 参数不同且指定时间内多次提交

   - POST 增量修改[减库存]: 基于现在的数据库数据更新

5. 解决方案

   1. token 机制: 验证码
      - 先删除令牌 + `redisget&del` 是原子性的
   2. 数据库锁: 悲观锁[for update] + 乐观锁[version]
   3. 数据库唯一性约束: 微服务间可以提供唯一 Id 进行判别[req-id 或者业务 id]
   4. 业务幂等

   ```sql
   -- 比如订单系统, 其中的扣款操作的幂等性设计:
   -- 可以通过设置 paystatus & orderId 来做幂等性
   -- 如果同一个订单的调用两次扣款操作, 第一次扣款成功之后则 paystatus 修改为 paid, 第二次执行以下 sql 则没有影响

   -- update userAmount set amount = amount - 'value', paystatus = 'paid' where orderId= 'orderid' and paystatus = 'unpay'
   ```

6. practice: 指定时间内同一个人的同样请求的参数出现大于 1 次就会触发幂等

   - local: `<uri, <md5, count>>` count is stored in ConcurrentHashMap
   - redis: setnxex, key-md5, value-uri=token

## 分布式唯一 Id

- 作为数据基础所以一定要`高可用`, `高并发`

### ~~UUID~~

1. 太长, 占用空间太大
2. 不是自增的, 作为数据库主键会出现`也分裂问题`

### 数据库自增

1. 在分表时会出问题, 两张表各自自增, 就会重复的 Id
2. 因此可以使用另外一张表去获取自增的 Id 使用

   ```sql
    create table seq_no(
        id bigint(20) unsigned not null auto_increment,
        stub char(10) not null default '',
        primary key (id)
    ) engine=InnoDB;
    -- necessary
    create unique index IDX_STUB on seq_no(stub)

    replace into seq_no(stub) value('anyword');
    select * from seq_no
    select last_insert_id();
   ```

3. `2 的问题`: 高可用下所有的数据库都应该是高可用的

   - 主从同步的延迟问题: 因此可能会出现同步丢失导致的重复问题
   - 主主模式下: 可以通过修改主键起始值和步长控制[使得每台 server 产生的 Id 不同]; 扩容时很难修改`[可以在上层加一个 service 去获取 Id]`

4. 缺点: 每次获取一个 Id 需要访问一次数据库

### 号段模式

1. 基于`数据库自增` 的且是一次获取一批 id[维护在内存中(有一个上层 DistributIdService)]
2. sql

   ```sql
   CREATE TABLE id_generator (
       id int(10) NOT NULL,
       current_max_id bigint(20) NOT NULL COMMENT '当前最大id',
       increment_step int(10) NOT NULL COMMENT '号段的长度',
       PRIMARY KEY (`id`)
   ) ENGINE=InnoDB DEFAULT CHARSET=utf8;
   ```

3. DistributIdService 是集群的, 数据库连接的是同一个数据库, 可能出现多个实例同时请求号段

   ```sql
   -- newMaxId是DistributIdService中根据oldMaxId+号段的长度算出来的; failed 之后则先获取一下 current_max_id 再计算 newMaxId
   update id_generator set current_max_id=#{newMaxId}, version=version+1 where version = #{version}
   ```

4. 数据库可以是多主模式: 通过 id 初始值和自增步长控制每台 server 返回的 Id 都不一样

### 雪花 ID

![avatar](/static/image/common/uuid-snow.png)

1. 让负责生成分布式 ID 的每台机器在每毫秒内生成不一样的 ID 就行了
2. 结构

   - 64 bit 的正整数: long 类型
   - 1bit: 固定为 0, 二进制中最高位为符号位, 0 为整数, 1 位负数.所以固定为 0 表示生成的 ID 都为正数
   - 41bit: 作为毫秒数, 大约能用 69`(1L << 41) / (1000L * 60 * 60 * 24 * 365)` 年; 存储的是时间戳的差值`当前时间-固定的开始时间`
   - 10bit: **作为机器编号**` [5bit 是数据中心 ID, 5bit 为机器 ID``很难实践 `]. 支持 1204 个实例.
   - 12bit: 序列号, 一毫秒最多生成 2^12=4096 个.

3. 优点: 递增, 且按时间有序. 性能高, 可根据情况分配 bit
4. 缺点:
   - 依赖机器时钟: 机器时钟回拨时可能出现重复 ID
   - 机器 Id[worker] 在集群模式下很难实践

### 百度[uid-generator](https://github.com/baidu/uid-generator/blob/master/README.zh_cn.md)

![avatar](/static/image/common/uuid-baidu.png)

1. uid-generator 使用的就是 snowflake, 只是在生产机器 id, 也叫做 workId 时有所不同
2. 结构

   - 64 bit 的正整数: long 类型
   - 1bit: 固定为 0 表示生成的 ID 都为正数
   - 28bit: 存储的是时间秒数的差值`当前时间-固定的开始时间`: 时间基点"2016-05-20"的增量值[8.7]
   - 22bit: workId, `同一个应用每重启一次就会消费一个workId`
   - 13bit: 序列号, 一毫秒最多生成 2^13=8192 个.

3. workId 的生成
   - 默认提供的策略: 应用启动时由数据库分配
   - 应用在启动时会往数据库表(uid-generator 需要新增一个 WORKER_NODE 表)中去插入一条数据, 数据插入成功后返回的该数据对应的自增唯一 id 就是该机器的 workId, 而数据由 host, port 组成

### 美团[Leaf]

1. snowflake 的一种 + 支持号段模式
2. workId 的生成
   - Leaf 中 workId 是基于 ZooKeeper 的顺序 Id 来生成的
   - 在启动时都会都在 Zookeeper 中生成一个顺序 Id, 相当于一台机器对应一个顺序节点, 也就是一个 workId

### Redis

1. 使用 incr, 但是需要考虑持久化的问题导致的 id 重复

## [Reactor](https://blog.csdn.net/larry_zeng1/article/details/78867992)

0. C10K 问题
1. Event + Handler: 对 IO 时间的抽象[fd 的抽象] + 回调处理器
   - 事件驱动: 事件触发的时执行(比如你的工资到账的时候, 有短信通知你, 而不是疯狂查看余额)
   - 不 IO 复用时, 判断读事件则只能阻塞; IO 复用可以在内核层面通知我们 --> multiplexer
   - 将 Acceptor 注册进去 demultiplexer 中; 当有连接到来时, Acceptor 被触发, 然后触发 Acceptor 的 handler, 获得新的 fd, 包装成事件, 重新注册成新的事件, 再次被注册到 demultiplexer 中; 然后接连处理
2. demultiplexer[分离器]: 对 epoll/select 的抽象

3. Reactor[反应堆]

   - 当一个事件开始驱动时, 就会陆续驱动多个事件, 最后就像核反应堆一样, 从而实现`高效地处理并发`
   - 核心: `IO复用+非阻塞编程+bind/function`
   - 至于如何拆分事件和粒度则不是 reactor 的思想[设计原则]

4. 举例说明: `TcpConnection = receive - process - send`

   - 正常处理, 要求当前线程必须处理完整个过程才能处理下个连接
   - Reactor 可以将其拆分成独立的: 处理 receive, process, send 彼此完全解耦, 只是有顺序关系
     - 当前线程处理完 receive, 然后将 process 注册到事件中, 然后处理下个连接

5. select 的问题: `解决一个线程处理多个连接的问题`

   ```c
   // r表示我们对哪些fd的可读事件感兴趣
   // w表示我们对哪些fd的可写事件感兴趣
   select(int nfds, fd_set *r, fd_set *w, fd_set *e, struct timeval *timeout)
   // 调用前这3个集合表示我们感兴趣的事件, 调用后这3个集合表示实际发生的事件
   ```

   - r,w,e 的 FD_SETSIZE 大小: 通常为 1024
   - 由于这 3 个集合在返回时会被内核修改， 因此我们`每次调用时都需要重新设置`
   - 在调用完成后需要全量的扫描这 3 个集合才能知道哪些 fd 的读/写事件发生: 效率比较低下
   - `内核`的每次调用都需要`扫描传递[无状态]`这 3 个 fd 集合, 看 fd 的事件是否发生: 存在效率问题

6. poll

   ```c
   poll(struct pollfd *fds, int nfds, int timeout)

   struct pollfd {
      int fd;
      // 感兴趣的事件
      short events;
      // 实际发生的事件
      short revents;
   }
   ```

   - poll 没有传递固定大小的 bitmap: select 1 的问题解决
   - poll 将感兴趣的事件和实际的事件分开: select 2 的问题解决
   - 问题 3 没有解决: select 问题 3 比较容易解决, `只要系统调用返回的是实际发生相应事件的fd集合`
   - 问题 4 没有解决: 无状态 --> 有状态[在第一次调用的时候记录这些 fd, 然后我们在以后的调用中不需要再传这些 fd]

7. epoll ~~& kqueue~~: `有状态`

   ```c
   // 创建一个context: 状态保存者
   int epoll_create(int size);
   // 将新的感兴趣的fd的读/写事件更新到context中
   int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
   // 等待context中fd的事件发生
   int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);
   ```

   - 解决了 select 的问题

   ![avatar](/static/image/common/reactor-common.png)

8. reactor & redis

   - 事件分类: 文件事件和时间事件
   - 文件事件: `服务器对套接字操作的抽象`

     1. 客户端的 GET 请求对于服务器来说就是一个文件事件

   - 时间事件: `服务器定时或周期性执行的事件`

     1. RDB 持久化

   - core code

     ```c#
     int main(int argc, char **argv) {
        ...
        // 建立各个事件处理器
        initServer();
        ...
        // 执行事件处理循环
        aeMain();
        ...
        // 关闭停止事件处理循环
        aeDeleteEventLoop(server.el);
        return 0;
     }
     ```
