## [简述 paxos 算法](https://www.bilibili.com/video/BV1kA411G7cK)

1. 简介:

   - **一个分布式系统如何就某个值达成一致**
   - 分布式数据系统的初始值相等且执行相等的操作系列, 最后也是一致的
   - 前提: 通信可靠不会被篡改
   - 是理论指导, 不是解决方案, 不能直接使用, 但是 raft, zab 等具体的落地方案

2. 角色介绍: `一个进程可以充当不同的角色`

   - proposer: 向集群提出议案 + 在发生冲突时调节
   - acceptor: 对提议者进行投票, 只有达到大多数时才会被最终接受
   - learner: 提议接受者, 单纯记录人员, acceptor 告诉那个 value 被共识了他就记录一下, 并给其他未共识的节点广播
   - proposal: proposer 提出的议案或者建议[编号为 n 和内容 value]
   - 最终的目标: 每个 proposer， acceptor, learner 都认为同一个 proposal 中的值被选中

3. 分类

   - basic paxos: 一轮只能确定一个值
   - multi-paxos: 一轮只能确定多个值
   - fast-paxos: zookeeper

4. 缺点
   - 活锁: prepare 可以成功, 但是 accept 时一直可能失败
     1. proposer 的 proposal 被拒绝之后, 会采纳 acceptor 的更大编号进行提交
     2. 如果 两个 proposer 都发现自己自己编号低转而提出更大的 n 就会导致死循环
     3. 解决: 二进制指数退避算法[时间等待]
   - 效率低下: 完整流程需要两大轮返回请求 + `multi-paxos 先选举出leader之后每次就只需要一轮`
   - 实现困难: 共识算法的公有问题

### basic paxos

1. prepare 阶段
   - prepare(n) 请求:
     1. proposer[在收到客户端请求或者发现本地有未提交的值{宕机}] 则提出一个编号为 n(自己已知递增) 的议案
     2. 向超过半数的/所有的 acceptor 广播, `只有编号`
   - **promise(n, value) 返回: acceptor 收到一个编号为 n 的 prepare 请求** 这里还是优点问题的: `有承若的请求 --> 已承若请求和N比 --> 是否有已提交的方案`
     1. 如果 acceptor 已经有已提交的 value 记录, 对比记录的编号和 N, 大于 N 则拒绝回应， 此否则返回该记录的 value 即编号
     2. 如果 acceptor 已经没已提交的 value 记录, 判断本地是否有编号 N1, 如果 N1 > N 则拒绝回应, 否则将 N1 改为 N 后并响应 prepare
2. accept 阶段

   - accept(n, value) 请求: proposer 收到多数派[多余一半的 acceptor 返回的 promise]
     1. 如果所有的 promise 返回的都是 N 则使用 N 对应的值作为 value, 否则会使用返回中最大编号对应的值作为 value
     2. proposer 会给半数以上的 acceptor 发送一个 accept(n, value`)的请求
     3. acceptor 对比 N 和自己承若的请求, 大于 N 则拒绝响应; 否则接受提案+广播 accept(n, value)
   - 当 proposer 收到超过半数的 acceptor 的返回值之后, 达成共识, 采用 N 编号对应的值
   - 并且同步给 learner 使未响应的 acceptor 达成一致

   ![avatar](/static/image/common/mic-paxos-flow-v2.png)
   ![avatar](/static/image/common/mic-paxos.png)

### multi-paxos

1. proposer-A 的 accept(n, value) 成功之后, 则让其他节点在一段时间内不能 accept 请求
2. 这样的话后面 proposer-A 就可以不需要经过 prepare 确认, 直接 accept: `重复使用该编号进行值修改{相当于时一个 leader}`
3. 直到某一个 accept 请求因超时而失败, 则会重新发起 prepare 进行 leader 的选举[accept 成功]

---

## 简述 raft 算法

1. raft 算法时对 paxos 理论的落地实现, 可以进行工程化的应用
2. 简介

   - 分布式一致性算法
   - raft 会先选举 leader, leader 完全负责 replicated log 的管理
   - leader 接受所有客户端的更新请求[follower 收到请求必须转给 leader], 然后赋值到从节点[合适的时机同步]
   - 如何 leader 挂了, 从节点会再次选举出新的 leader

3. 三种状态: 一个节点在任意时刻只能时其中的一种

   - leader: 处理所有客户端请求, 并负责同步数据给从节点
   - follower: 不会发送任何请求, 只会简单的响应 leader 和 candidate 的请求
   - candidate: 用于选举产生新的 leader

4. term: 任期, 从 leader 产生到重新选举为一任期, 每个节点都有当前任期的任期号

   - term 是递增的, 存储在 log 日志的 entry 中, 代表当前 entry 是哪一个任期内写入的
   - 每个任期最多只能有 1 个 leader, 可以没有[选举失败]
   - leader 每次 rpc 通信时传递任期号, 如果 rpc 收到的任期号大于本地的, 则切换为 follower; 小于本地任期号则返回错误信息

5. 两个 rpc 通信

   - request vote rpc: 负责选举, 包含参数 **lastIndex**[已同步的数据量], lastTerm
   - append entries rpc: 负责数据的交互

6. 日志序列

   - 每个节点上维护一个持久化的顺序存放的 log
   - 通过一致性算法, 保证每个节点中的 log 是一致的
   - 客户端可以在每个节点上读到相等的数据

7. 日志序列的同步过程: 日志需要持久化到磁盘, 崩溃之后可以从日志中恢复

   - 客户端发送命令给 leader
   - leader 把自己的日志条目加到日志序列中~~[大多数同意后才会写入]~~
   - leader 发送 append entries rpc 给所有的 follower: 携带 preLogIndex, preLogTerm, log
   - follower 收到后进行日志序列匹配: leader 跟每一个 follower 都有一个通道
     1. 匹配上则直接追加到自己的日志序列
     2. 匹配不上则拒绝请求, leader 将日志的 preLogIndex 调小, 直至匹配上, follower 获取 leader 的所有日志序列
     3. follower 的日志 preLogIndex 大于 leader 的就删除[之前可能时主节点]`[follower 日志有跳跃的直接删除跳跃日志]`
   - leader 发现一旦该日志序列同步到大多数节点了, 则将日志应用到状态机中

     1. leader 在状态机中提交自己的日志序列条目, **然后返回结果给客户端**
     2. leader 下次发送 append entries rpc 时告知 follower 已提交的日志序列条目信息 lastIndex
     3. follower 收到 rpc 之后提交到自己的状态机中

   - **提交状态机时如果数据是上一任期 term 的`[leader, data, 下线, 有上线了]`, 不能直接提交, 必须等待当前任期的一次数据提交一起才能提交**

     1. 此时日志序列号大, 别人很难被选举出来
     2. 提交上一任期 term 的数据可能导致**写入日志序列并提交到状态机中**的数据被覆盖

   - 安全原则

8. leader 和 follower 同步数据的安全原则

   - 新选举出来的 leader 一定拥有所有已提交状态机的日志序列条目: leader 在同步到大多数才会提交到状态机 + 被选举出来的 leader 的 lastIndex 必须大于大多数节点
   - 选举安全原则: 对于一个给定的状态机, 最多只会有一个 leader 被选举出来
   - 状态机安全原则: 如果 leader 将指定位置的日志序列条目提交到状态机, 其他服务器的此位置都会是相同的日志序列条目
   - leader 完全原则: 如果某个日志条目在某个任期中被提交到状态机, 则该条目必然出现在更大任期好的所有 leader 中`[7-6]`
   - leader 只附加原则: leader 对不不会删除或者覆盖自己的日志条目, 只会增加
   - 日志匹配原则: 如果两个日志在相同位置的任期号一样, 我们就认为到此位置的日志条目完全相同

9. 状态机

   - 日志序列同步到**多数节点时**, leader 将该日志提交到状态机
   - 并在下一次心跳通知所有节点提交状态[携带最后提交的 lastIndex]

10. 触发选举时机

    - 集群初始化, 都是 follower, 随机超时[sleep], 变成 candidate[最先醒过来的那一个] 后发生选举
    - 如果 follower 在 election timeout 内没收到来自 leader 的心跳, 则主动触发选举

11. 选举过程: 发起选举节点角度

    - 增加本地节点的 term, 切换到 candidate 状态
    - 投自己一票
    - 并行给其他节点发送 request vote rpcs 请求: 包好 term, lastIndex 参数
    - 其他节点的选举逻辑: 每个节点同一任期只能投一票, 候选人的已同步的数据量不能比自己少`[副本机制+安全机制]`, 先到先得
    - 等待回复

      1. 若收到大多数的投票, 赢得选举, 切换到 leader 状态, 立即给所有节点发送心跳信息
      2. 若被告知别人当选, 则切换为 follower 状态: 若是原来的 leader 则对比 term, 比自己的大, 则原来的 leader 切换为 follower
      3. 一段时间内没收到大多数的投票或新 leader 的心跳, 则保持 candidate 重新发起选举

12. 脑裂问题: 多个 leader

    - leader 网络不通, 集群重新选举出新的 leader
    - 原来的 leader 由回复网络了, 此时有两个 leader 则称之为脑裂
    - termid 可以解决这个问题, 原来的 leader 回来去发送请求时发现 term 比别人小, 则变成 follower

---

## 简述 zab 算法
