## [接口幂等性](https://github.com/Alice52/java-ocean/issues/201)

1. 定义: 就是任意多次执行所产生的影响均与一次执行的影响相同
2. 场景

   - Feign 的 retry 机制
   - 用户多次点击
   - 重复提交[连续点了 2 次/web 端回退导致的]
   - 有一些操作用户退出了, 又重新点击进来: 放大了时间差

3. 操作本身具有幂等性

   - 带主键的插入,
   - 查询
   - 定量更新
   - 删除操作

4. 操作不具有幂等性的: 参数不同且指定时间内多次提交

   - POST 增量修改[减库存]: 基于现在的数据库数据更新

5. 解决方案

   1. token 机制: 验证码
      - 先删除令牌 + `redisget&del` 是原子性的
   2. _数据库锁: 悲观锁[for update] + 乐观锁[version]_
   3. 数据库唯一性约束: 微服务间可以提供唯一 Id 进行判别[req-id 或者业务 id]
   4. 业务幂等: 状态机幂

   ```sql
   -- 比如订单系统, 其中的扣款操作的幂等性设计:
   -- 可以通过设置 paystatus & orderId 来做幂等性
   -- 如果同一个订单的调用两次扣款操作, 第一次扣款成功之后则 paystatus 修改为 paid, 第二次执行以下 sql 则没有影响

   -- update userAmount set amount = amount - 'value', paystatus = 'paid' where orderId= 'orderid' and paystatus = 'unpay'
   ```

6. practice: 指定时间内同一个人的同样请求的参数出现大于 1 次就会触发幂等

   - local: `<uri, <md5, count>>` count is stored in ConcurrentHashMap
   - redis: setnxex, key-md5, value-uri=token

## 分布式唯一 Id

- 作为数据基础所以一定要`高可用`, `高并发`

### ~~UUID~~

1. 太长, 占用空间太大
2. 不是自增的, 作为数据库主键会出现`页分裂问题`

### 数据库自增

1. 在分表时会出问题, 两张表各自自增, 就会重复的 Id
2. 因此可以使用另外一张表去获取自增的 Id 使用

   ```sql
    create table seq_no(
        id bigint(20) unsigned not null auto_increment,
        stub char(10) not null default '',
        primary key (id)
    ) engine=InnoDB;
    -- necessary
    create unique index IDX_STUB on seq_no(stub)

    replace into seq_no(stub) value('anyword');
    select * from seq_no
    select last_insert_id();
   ```

3. `2 的问题`: 高可用下所有的数据库都应该是高可用的

   - 主从同步的延迟问题: 因此可能会出现同步丢失导致的重复问题
   - 主主模式下: 可以通过修改主键起始值和步长控制[使得每台 server 产生的 Id 不同]; 扩容时很难修改`[可以在上层加一个 service 去获取 Id]`

4. 缺点: 每次获取一个 Id 需要访问一次数据库

### 号段模式

1. 基于`数据库自增` 的且是一次获取一批 id[维护在内存中(有一个上层 DistributIdService)]
2. sql

   ```sql
   CREATE TABLE id_generator (
       id int(10) NOT NULL,
       current_max_id bigint(20) NOT NULL COMMENT '当前最大id',
       increment_step int(10) NOT NULL COMMENT '号段的长度',
       PRIMARY KEY (`id`)
   ) ENGINE=InnoDB DEFAULT CHARSET=utf8;

   -- 可以只有一条记录: 每次都是做更新{数据库内有行锁保证线程安全}
   -- 也可以 insert + 悲观锁
   insert into id_generator(current_max_id, increment_step) values(1, 2000)
   ```

3. DistributIdService 是集群的, 数据库连接的是同一个数据库, 可能出现多个实例同时请求号段

   ```sql
   -- newMaxId是DistributIdService中根据oldMaxId+号段的长度算出来的; failed 之后则先获取一下 current_max_id 再计算 newMaxId
   update id_generator set current_max_id=#{newMaxId}, version=version+1 where version = #{version}
   ```

4. 数据库可以是多主模式: 通过 id 初始值和自增步长控制每台 server 返回的 Id 都不一样

### 雪花 ID

![avatar](/static/image/common/uuid-snow.png)

1. 让负责生成分布式 ID 的每台机器在每毫秒内生成不一样的 ID 就行了
2. 结构

   - 64 bit 的正整数: long 类型
   - 1bit: 固定为 0, 二进制中最高位为符号位, 0 为整数, 1 位负数.所以固定为 0 表示生成的 ID 都为正数
   - 41bit: 作为毫秒数, 大约能用 69`(1L << 41) / (1000L * 60 * 60 * 24 * 365)` 年; 存储的是时间戳的差值`当前时间-固定的开始时间`
   - 10bit: **作为机器编号**` [5bit 是数据中心 ID, 5bit 为机器 ID``很难实践 `]. 支持 1204 个实例.
   - 12bit: 序列号, 一毫秒最多生成 2^12=4096 个.

3. 优点: 递增, 且按时间有序. 性能高, 可根据情况分配 bit
4. 缺点:
   - 依赖机器时钟: 机器时钟回拨时可能出现重复 ID
   - 机器 Id[worker] 在集群模式下很难实践

### 百度[uid-generator](https://github.com/baidu/uid-generator/blob/master/README.zh_cn.md)

![avatar](/static/image/common/uuid-baidu.png)

1. uid-generator 使用的就是 snowflake, 只是在生产机器 id, 也叫做 workId 时有所不同
2. 结构

   - 64 bit 的正整数: long 类型
   - 1bit: 固定为 0 表示生成的 ID 都为正数
   - 28bit: 存储的是时间秒数的差值`当前时间-固定的开始时间`: 时间基点"2016-05-20"的增量值[8.7]
   - 22bit: workId, `同一个应用每重启一次就会消费一个workId`
   - 13bit: 序列号, 一毫秒最多生成 2^13=8192 个.

3. workId 的生成

   - 默认提供的策略: 应用启动时由数据库分配
   - 应用在启动时会往数据库表(uid-generator 需要新增一个 WORKER_NODE 表)中去插入一条数据, 数据插入成功后返回的该数据对应的自增唯一 id 就是该机器的 workId, 而数据由 host, port 组成

4. 简介

   - 最终单机 QPS 可达 600 万
   - 适用于 docker 等虚拟化环境下实例自动重启、漂移等场景
   - UidGenerator 借用`未来时间`来解决 sequence 天然存在的并发限制:
     1. 采用 RingBuffer{内部两个环形数组} 来 {缓存已生成的 UID, 标识是否可填充|消费}: 并行化 UID 的生产和消费
     2. 同时对 CacheLine 补齐, 避免了由 RingBuffer 带来的硬件级「伪共享」问题

5. 生成 UID 的过程: `Uid-RingBuffer[存Uid] + Flag-RingBuffer[存Uid状态]`

   - 根据配置初始化 RingBuffer`环形数组` 容量: 默认为 Snowflake 算法中 sequence 最大值[且为 2^N]
   - 并在 RingBuffer of UID 上填充 uid: 数组是一个`只读的+批量写的`环
   - 且 RingBuffer of Flag 上填充 uid 对应的是否可{PUT/TAKE}的状态: 数组是一个读写的环
   - 当 Flag 环的使用率达到一定值就会在 UID 的环上从尾 Tail 开始填充环, 并将 Flag 对应的位置改为可用
   - 获取 uid 时是读取 flag 的环: 头 cursor 开始并下移[顺时针]一位
   - core code

     ```java
     // slots 与 flags 之间没有锁: 先设置 flag, 之后在设置 slots{不需要锁}

     // uuid + 多线程批量填充[次数不多, 所以没有解决伪共享问题]
     private final long[] slots;

     // 每次获取uid都会写, 频繁的写会导致伪共享{cacheline失效}问题: 所以使用 paddinglong 解决该问题{本质上是让一个flag占满一个缓存行}
     // uuid flag and FalseSharing
     //  The CPU cache line commonly be 64 bytes, here is a sample of cache line after padding:
     //     64 bytes = 8 bytes (object reference) + 6 * 8 bytes (padded long) + 8 bytes (a long value)
     //     todo: 64 bytes = 8 bytes(_mark) + 8[/4] byte(*cp) + 5 * 8 bytes(padded long) + 8 bytes(a long value)
     private final PaddedAtomicLong[] flags;
     ```

6. RingBuffer 填充时机

   - 初始化预填充: **RingBuffer 初始化时, 预先填充满整个 RingBuffer**
   - 即时填充
     1. Take 消费时, 即时检查剩余可用 slot 量(tail - cursor)
     2. 如小于设定阈值, 则补全空闲 slots
     3. 阈值可通过 paddingFactor 来进行配置
   - 周期填充
     1. 通过 Schedule 线程, 定时补全空闲 slots
     2. 可通过 scheduleInterval 配置, 以应用定时填充功能, 并指定 Schedule 时间间隔

7. 环满了的拒绝策略 || 环空的拒绝策略
8. RingBuffer 的特点

   - 数组元素在内存中是连续分配的, 可最大程度利用 CPU cache 以提升性能
   - **可以使用缓存行 L1/2/3, 缓存行大小一般是 64 个字节, slot 是 8 个字节[long], 所以一次能读 8 个 uuid 到缓存中, 读 uuid 的速度直接走缓存**
   - 但缓存行**同时会带来「伪共享」 FalseSharing 问题**:

     1. 写的时候会互相竞争数据的写权限, 导致变慢[伪共享{只有写操作}]
     2. **cpu1 和 cpu2 竞争胜利的去写, 另一个在缓存中的数据就作废了**
     3. 数据每个线程都需要写[一个线程写前 8 个字节, 第二个线程写后 8 个字节], 两个线程竞争产生`伪共享`

   - 伪共享解决: 为此在 Tail、Cursor 指针、Flag-RingBuffer 中采用了 CacheLine[64] 补齐方式[PaddedAtomicLong]
   - 并发解决是 tail 和 cursor 都是 atomicLong 解决的

#### [伪共享](https://www.cnblogs.com/tong-yuan/p/FalseSharing.html)

- 本质就是 **`不同核心多个线程修改互相独立的且存在于一个缓存行的变量, 影响导致彼此性能`**
- 原因: cpu 修改变量之后会写回该核心的 L1/L2/L3 缓存, 之后再写入主内存, 并失效其他包含此变量的缓存[获取时就需要再次去一层一层获取], 而且可能会导致相互挤兑的问题
- **[L1/L2](https://www.cnblogs.com/cyfonly/p/5800758.html) 每个核心都有, L3 是所有核心公用**: `拥有三级缓存的的 CPU, 到三级缓存时能够达到 95% 的命中率, 只有不到 5% 的数据需要从内存中查询`

1. CPU 缓存行:

   - CPU 缓存[Cache Memory]是位于 CPU 与内存之间的临时存储器, 它的容量比内存小的多但是交换速度却比内存要快得多
   - CPU 与优先与缓存交互: L1/L2/L3`[只有L3是所有core公用的]`
   - 一般是 64byte: `一个缓存行存储大小`
   - CPU 获取缓存中数据的流程是: cpu -> L1 -> L2 -> L3 -> 主内存
   - 缓存行带来好处
     1. 如果访问一个 long 类型的数组时, 当数组中的一个值被加载到缓存中时, 另外 7 个元素也会被加载到缓存中
     2. 但是, 如果使用的数据结构中的项在内存中不是彼此相邻的, 比如链表, 那么将得不到免费缓存加载带来的好处
   - 免费加载也有一个坏处: 就是伪共享
     1. 设想 a, b 两个数据在同一个缓存行内
     2. 线程 A 去修改 a, 会把 a, b 都加载到缓存行中;
     3. 线程 A 修改完会写回 L1, L2, L3, 并写回主内存 + 通知其他包含此缓存行的 cpu 核心失效
     4. 同核心的线程 B 读取 b, 在 L1 内直接命中: `不存在伪共享`
     5. **不同核心的线程 B 读取 b[即使之前已经加载到缓存中也会被 A 失效], 需要重新去 L1 -> L2 -> L3(找到) 获取**

2. `AtomicLong + 伪共享在 uid 中的作用`

   - AtomicLong: 判断出每个 CPU 获取的数组下标 && 且**每个元素都占有一个缓存行**
   - cpu core1 需要缓存行 1
   - cpu core2 需要缓存行 2
   - core1 去修改 1 将其变为已获取, 之后会通知其他包含此值的缓存行都失效
   - core3 去修改 3 将其变为已获取, 之后会通知其他包含此值的缓存行都失效[在此过程中缓存行 1 也有可能被失效]

3. 结论

   - CPU 具有多级缓存，越接近 CPU 的缓存越小也越快
   - CPU 缓存中的数据是以缓存行为单位处理的
   - CPU 缓存行能带来免费加载数据的好处, 所以处理数组性能非常高
   - CPU 缓存行也带来了弊端, 多线程处理不相干的变量时会相互影响，也就是伪共享
   - 避免伪共享的主要思路就是让不相干的变量不要出现在同一个缓存行中
   - 一是每两个变量之间加七个 long 类型
   - 二是创建自己的 long 类型，而不是用原生的`[LongAdder]`

     ```java
     // -XX:-RestrictContended 才会使得 Contended 生效
     @sun.misc.Contended
     class MyLong {
        volatile long value;
     }
     ```

   - 三是使用 java8 提供的注解

4. jdk 中的应用

   - ConcurrentHashMap: size() 方法使用的是分段的思想来构造的, 每个段使用的类是 CounterCell, 它的类上就有 @sun.misc.Contended 注解
   - LongAdder

### 美团[Leaf](https://tech.meituan.com/2019/03/07/open-source-project-leaf.html)

1. snowflake || 支持号段模式
2. 号段模式

   - sql

   ```sql
   -- 每个业务下一条记录: 统一业务下多个线程安全问题是靠 数据库行锁解决的
   CREATE TABLE `leaf_alloc` (
      `biz_tag` varchar(128)  NOT NULL DEFAULT '',
      `max_id` bigint(20) NOT NULL DEFAULT '1',
      `step` int(11) NOT NULL,
      `description` varchar(256)  DEFAULT NULL,
      `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
      PRIMARY KEY (`biz_tag`)
   ) ENGINE=InnoDB;
   ```

   - 数据库 --> 内存 buffer{两个[异步触发填充下一个]} --> 从 buffer 里分配 ID{atomicLong}
   - buffer 内用完了, 是使用 atomicbool 标识位让一个线程去填充: 其他线程在 while(true)

3. snowflake 中 workId 的生成
   - Leaf 中 workId 是基于 ZooKeeper 的顺序 Id 来生成的
   - 在启动时都会都在 Zookeeper 中生成一个顺序 Id, 相当于一台机器对应一个顺序节点, 也就是一个 workId
   - Leaf 在第一次从 Zookeeper 拿取 workerID 后, 会在本机文件系统上缓存一个 workerID 文件: 即使 ZooKeeper 出现问题, 同时恰好机器也在重启, 也能保证服务的正常运行

### Redis

1. 使用 incr, 但是需要考虑持久化的问题导致的 id 重复

### Zookeeper

1. sequence node: 可以解决问题, 但有并发压力

## [Reactor](https://blog.csdn.net/larry_zeng1/article/details/78867992)

0. C10K 问题
1. Event + Handler: 对 IO 时间的抽象[fd 的抽象] + 回调处理器
   - 事件驱动: 事件触发的时执行(比如你的工资到账的时候, 有短信通知你, 而不是疯狂查看余额)
   - 不 IO 复用时, 判断读事件则只能阻塞; IO 复用可以在内核层面通知我们 --> multiplexer
   - 将 Acceptor 注册进去 demultiplexer 中; 当有连接到来时, Acceptor 被触发, 然后触发 Acceptor 的 handler, 获得新的 fd, 包装成事件, 重新注册成新的事件, 再次被注册到 demultiplexer 中; 然后接连处理
2. demultiplexer[分离器]: 对 epoll/select 的抽象

3. Reactor[反应堆]

   - 当一个事件开始驱动时, 就会陆续驱动多个事件, 最后就像核反应堆一样, 从而实现`高效地处理并发`
   - 核心: `IO复用+非阻塞编程+bind/function`
   - 至于如何拆分事件和粒度则不是 reactor 的思想[设计原则]

4. 举例说明: `TcpConnection = receive - process - send`

   - 正常处理, 要求当前线程必须处理完整个过程才能处理下个连接
   - Reactor 可以将其拆分成独立的: 处理 receive, process, send 彼此完全解耦, 只是有顺序关系
     - 当前线程处理完 receive, 然后将 process 注册到事件中, 然后处理下个连接

5. select 的问题: `解决一个线程处理多个连接的问题`

   ```c
   // r表示我们对哪些fd的可读事件感兴趣
   // w表示我们对哪些fd的可写事件感兴趣
   select(int nfds, fd_set *r, fd_set *w, fd_set *e, struct timeval *timeout)
   // 调用前这3个集合表示我们感兴趣的事件, 调用后这3个集合表示实际发生的事件
   ```

   - r,w,e 的 FD_SETSIZE 大小: 通常为 1024
   - 由于这 3 个集合在返回时会被内核修改, 因此我们`每次调用时都需要重新设置`
   - 在调用完成后需要全量的扫描这 3 个集合才能知道哪些 fd 的读/写事件发生: 效率比较低下
   - `内核`的每次调用都需要`扫描传递[无状态]`这 3 个 fd 集合, 看 fd 的事件是否发生: 存在效率问题

6. poll

   ```c
   poll(struct pollfd *fds, int nfds, int timeout)

   struct pollfd {
      int fd;
      // 感兴趣的事件
      short events;
      // 实际发生的事件
      short revents;
   }
   ```

   - poll 没有传递固定大小的 bitmap: select 1 的问题解决
   - poll 将感兴趣的事件和实际的事件分开: select 2 的问题解决
   - 问题 3 没有解决: select 问题 3 比较容易解决, `只要系统调用返回的是实际发生相应事件的fd集合`
   - 问题 4 没有解决: 无状态 --> 有状态[在第一次调用的时候记录这些 fd, 然后我们在以后的调用中不需要再传这些 fd]

7. epoll ~~& kqueue~~: `有状态`

   ```c
   // 创建一个context: 状态保存者
   int epoll_create(int size);
   // 将新的感兴趣的fd的读/写事件更新到context中
   int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
   // 等待context中fd的事件发生
   int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);
   ```

   - 解决了 select 的问题

   ![avatar](/static/image/common/reactor-common.png)

8. reactor & redis

   - 事件分类: 文件事件和时间事件
   - 文件事件: `服务器对套接字操作的抽象`

     1. 客户端的 GET 请求对于服务器来说就是一个文件事件

   - 时间事件: `服务器定时或周期性执行的事件`

     1. RDB 持久化

   - core code

     ```c#
     int main(int argc, char **argv) {
        ...
        // 建立各个事件处理器
        initServer();
        ...
        // 执行事件处理循环
        aeMain();
        ...
        // 关闭停止事件处理循环
        aeDeleteEventLoop(server.el);
        return 0;
     }
     ```

## 系统性能优化

1. 压测: 大体上查看每个请求的时间与性能 qps/95
2. 针对性能不达标的 api 进行分析
3. 查看 prod 的 cpu, 内存, 网速
4. 查看数据库监控: 有没有慢 sql
5. 查看这个 api 的具体代码: 分析慢的可能原因
6. 优化相关代码
7. 打 log 去测试环境观察
8. 如果优化不了, 就需要考虑是不是数据库设计的不合理或者业务本身就有问题[改表+冗余]
9. 使用 countdownlacth 去并发 sql/方法测试
